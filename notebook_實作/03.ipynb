{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to coder):\n",
      "\n",
      "Plot a chart of 'META' and 'MSFT' stock price current YTD. Save the result to a file named nvda_tesla.png\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcoder\u001b[0m (to user_proxy):\n",
      "\n",
      "To plot a chart of 'META' and 'MSFT' stock price current YTD, we can use the yfinance library to fetch the stock price data and the matplotlib library to plot the chart. First, we'll need to install the yfinance library if it's not already installed. Then, we can fetch the stock price data and plot the chart.\n",
      "\n",
      "Here's the Python code to do this:\n",
      "\n",
      "```python\n",
      "# filename: stock_price_chart.py\n",
      "import yfinance as yf\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Fetch the stock price data for 'META' and 'MSFT'\n",
      "meta_data = yf.download('META', start='2022-01-01')\n",
      "msft_data = yf.download('MSFT', start='2022-01-01')\n",
      "\n",
      "# Plot the stock price data\n",
      "plt.figure(figsize=(12, 6))\n",
      "plt.plot(meta_data['Close'], label='META')\n",
      "plt.plot(msft_data['Close'], label='MSFT')\n",
      "plt.title('META and MSFT Stock Price YTD')\n",
      "plt.xlabel('Date')\n",
      "plt.ylabel('Stock Price (USD)')\n",
      "plt.legend()\n",
      "plt.grid(True)\n",
      "plt.savefig('nvda_tesla.png')\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "Please execute the above Python code to fetch the stock price data and save the chart to the file named nvda_tesla.png.\n",
      "After executing the code, you can find the nvda_tesla.png file in your current directory.\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to coder):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Figure(1200x600)\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcoder\u001b[0m (to user_proxy):\n",
      "\n",
      "The code executed successfully and the chart has been saved to the file named nvda_tesla.png in your current directory. You can open the file to view the stock price chart of 'META' and 'MSFT' for the current year.\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to coder):\n",
      "\n",
      "Great! I'm glad to hear that the chart was successfully saved. If you have any more requests or need further assistance, feel free to ask.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_history=[{'content': \"Plot a chart of 'META' and 'MSFT' stock price current YTD. Save the result to a file named nvda_tesla.png\", 'role': 'assistant'}, {'content': \"To plot a chart of 'META' and 'MSFT' stock price current YTD, we can use the yfinance library to fetch the stock price data and the matplotlib library to plot the chart. First, we'll need to install the yfinance library if it's not already installed. Then, we can fetch the stock price data and plot the chart.\\n\\nHere's the Python code to do this:\\n\\n```python\\n# filename: stock_price_chart.py\\nimport yfinance as yf\\nimport matplotlib.pyplot as plt\\n\\n# Fetch the stock price data for 'META' and 'MSFT'\\nmeta_data = yf.download('META', start='2022-01-01')\\nmsft_data = yf.download('MSFT', start='2022-01-01')\\n\\n# Plot the stock price data\\nplt.figure(figsize=(12, 6))\\nplt.plot(meta_data['Close'], label='META')\\nplt.plot(msft_data['Close'], label='MSFT')\\nplt.title('META and MSFT Stock Price YTD')\\nplt.xlabel('Date')\\nplt.ylabel('Stock Price (USD)')\\nplt.legend()\\nplt.grid(True)\\nplt.savefig('nvda_tesla.png')\\nplt.show()\\n```\\n\\nPlease execute the above Python code to fetch the stock price data and save the chart to the file named nvda_tesla.png.\\nAfter executing the code, you can find the nvda_tesla.png file in your current directory.\\n\\nTERMINATE\", 'role': 'user'}, {'content': 'exitcode: 0 (execution succeeded)\\nCode output: \\nFigure(1200x600)\\n', 'role': 'assistant'}, {'content': \"The code executed successfully and the chart has been saved to the file named nvda_tesla.png in your current directory. You can open the file to view the stock price chart of 'META' and 'MSFT' for the current year.\\n\\nTERMINATE\", 'role': 'user'}, {'content': \"Great! I'm glad to hear that the chart was successfully saved. If you have any more requests or need further assistance, feel free to ask.\", 'role': 'assistant'}], summary=\"Great! I'm glad to hear that the chart was successfully saved. If you have any more requests or need further assistance, feel free to ask.\", cost=({'total_cost': 0.002515, 'gpt-3.5-turbo-1106': {'cost': 0.002515, 'prompt_tokens': 1755, 'completion_tokens': 380, 'total_tokens': 2135}}, {'total_cost': 0}), human_input=[])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat/\n",
    "AssistantAgent 設計為充當 AI 助手，預設使用LLMs但不需要人工輸入或代碼執行。它可以編寫 Python 代碼（在 Python 編碼塊中），供使用者在收到消息（通常是需要解決的任務的描述）時執行。在後台，Python 代碼是由LLM（例如 GPT-4）編寫的。它還可以接收執行結果並提出更正或錯誤修復建議。可以通過傳遞新的系統消息來更改其行為。LLM推理配置可以通過 [ llm_config ] 進行配置。\n",
    "UserProxyAgent 人類的代理，默認情況下，在每次交互時徵求人類輸入作為代理的回復，並且還具有執行代碼和調用函數或工具的能力。當它在接收到的消息中檢測到可執行代碼塊並且沒有提供人工使用者輸入時，它會自動 UserProxyAgent 觸發代碼執行。可以通過將 code_execution_config 參數設置為 False 來禁用代碼執行。\n",
    "默認情況下不是基於LLM的回覆(所以是誰回覆的???)，LLM-based response is disabled by default.。可以通過設置為 llm_config 與推理配置相對應的字典來啟用它。When llm_config 設置為字典，則當 UserProxyAgent 未執行代碼，就還是可以使用 LLM 來生成回復。\n",
    "\n",
    "＊＊＊＊＊通過下列程式的執行過程，autogen 調用 gpt3.5 時，連調用函式的程式碼似乎也都會自己生成(也就是靠自然語言就可讓gpt3.5產生要調用函式的程式碼，厲害)\n",
    "\"\"\"\n",
    "import autogen\n",
    "config_list = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-3.5-turbo-1106\"], # gpt-3.5-turbo-16k, gpt-3.5-turbo, gpt-3.5-turbo-1106\n",
    "    },\n",
    ")\n",
    "# 下面是另一種方式\n",
    "llm_config={\"config_list\": config_list, \"seed\": 42}\n",
    "\n",
    "coder = autogen.AssistantAgent(\"coder\", llm_config=llm_config, max_consecutive_auto_reply=2)\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\"user_proxy\", \n",
    "                                    llm_config=llm_config, \n",
    "                                    human_input_mode=\"NEVER\", # user_proxy 人工介入模式：NONE, NEVER, TERMINATE, ALWAYS 四種選項, 差異???\n",
    "                                    max_consecutive_auto_reply=2,\n",
    "                                    code_execution_config={\"work_dir\": \"coding\", \"use_docker\": False})\n",
    "\n",
    "task = \"Plot a chart of 'META' and 'MSFT' stock price current YTD. Save the result to a file named nvda_tesla.png\"\n",
    "user_proxy.initiate_chat(coder, message = task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "OpenAI.__init__() got an unexpected keyword argument 'api_version'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 41\u001b[0m\n\u001b[0;32m     31\u001b[0m config_list_gemini_vision \u001b[38;5;241m=\u001b[39m autogen\u001b[38;5;241m.\u001b[39mconfig_list_from_json(\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOAI_CONFIG_LIST\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     33\u001b[0m     filter_dict\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-pro-vision\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     35\u001b[0m     },\n\u001b[0;32m     36\u001b[0m )\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# 定義2個Agent，assistant和user_proxy\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# 但不清楚為何跟下面區塊直接設定使用哪個模型不一樣，這裡用 \"assistant\" 跟 \"user_proxy\"\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# 下面的區塊是用 \"gpt-3.5-turbo-1106\"、\"gemini-pro\",\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m assistant \u001b[38;5;241m=\u001b[39m \u001b[43mAssistantAgent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43massistant\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig_list\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_list_gpt35\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_consecutive_auto_reply\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\n\u001b[0;32m     43\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# print(assistant.system_message)\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# 這個 user_proxy 模型用 \"user_proxy\" ??? 不懂\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# 也沒有 llm_config，但還是可以執行???\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# UserProxyAgent 應該只能使用 openAI，其他會報錯，但這個區塊卻又可以????? 不過，整個來說，幾乎都是不行的。\u001b[39;00m\n\u001b[0;32m     48\u001b[0m user_proxy \u001b[38;5;241m=\u001b[39m UserProxyAgent(\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_proxy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     50\u001b[0m     code_execution_config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwork_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_docker\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m},\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     53\u001b[0m     is_termination_msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: content_str(x\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTERMINATE\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     54\u001b[0m )\n",
      "File \u001b[1;32md:\\TOMO.Project\\autogen\\venv\\lib\\site-packages\\autogen\\agentchat\\assistant_agent.py:61\u001b[0m, in \u001b[0;36mAssistantAgent.__init__\u001b[1;34m(self, name, system_message, llm_config, is_termination_msg, max_consecutive_auto_reply, human_input_mode, description, **kwargs)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     35\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     43\u001b[0m ):\n\u001b[0;32m     44\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;03m        name (str): agent name.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03m            [ConversableAgent](conversable_agent#__init__).\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     62\u001b[0m         name,\n\u001b[0;32m     63\u001b[0m         system_message,\n\u001b[0;32m     64\u001b[0m         is_termination_msg,\n\u001b[0;32m     65\u001b[0m         max_consecutive_auto_reply,\n\u001b[0;32m     66\u001b[0m         human_input_mode,\n\u001b[0;32m     67\u001b[0m         llm_config\u001b[38;5;241m=\u001b[39mllm_config,\n\u001b[0;32m     68\u001b[0m         description\u001b[38;5;241m=\u001b[39mdescription,\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     70\u001b[0m     )\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logging_enabled():\n\u001b[0;32m     72\u001b[0m         log_new_agent(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlocals\u001b[39m())\n",
      "File \u001b[1;32md:\\TOMO.Project\\autogen\\venv\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:154\u001b[0m, in \u001b[0;36mConversableAgent.__init__\u001b[1;34m(self, name, system_message, is_termination_msg, max_consecutive_auto_reply, human_input_mode, function_map, code_execution_config, llm_config, default_auto_reply, description)\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_config \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig_list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    149\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m config \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig_list\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    150\u001b[0m     ):\n\u001b[0;32m    151\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    152\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease either set llm_config to False, or specify a non-empty \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m either in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllm_config\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or in each config of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig_list\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    153\u001b[0m         )\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m OpenAIWrapper(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_config)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logging_enabled():\n\u001b[0;32m    157\u001b[0m     log_new_agent(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlocals\u001b[39m())\n",
      "File \u001b[1;32md:\\TOMO.Project\\autogen\\venv\\lib\\site-packages\\autogen\\oai\\client.py:373\u001b[0m, in \u001b[0;36mOpenAIWrapper.__init__\u001b[1;34m(self, config_list, **base_config)\u001b[0m\n\u001b[0;32m    371\u001b[0m     config_list \u001b[38;5;241m=\u001b[39m [config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mfor\u001b[39;00m config \u001b[38;5;129;01min\u001b[39;00m config_list]  \u001b[38;5;66;03m# make a copy before modifying\u001b[39;00m\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m config \u001b[38;5;129;01min\u001b[39;00m config_list:\n\u001b[1;32m--> 373\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_register_default_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopenai_config\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# could modify the config\u001b[39;00m\n\u001b[0;32m    374\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_list\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m    375\u001b[0m             {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopenai_kwargs}}\n\u001b[0;32m    376\u001b[0m         )\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\TOMO.Project\\autogen\\venv\\lib\\site-packages\\autogen\\oai\\client.py:426\u001b[0m, in \u001b[0;36mOpenAIWrapper._register_default_client\u001b[1;34m(self, config, openai_config)\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clients\u001b[38;5;241m.\u001b[39mappend(OpenAIClient(client))\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 426\u001b[0m     client \u001b[38;5;241m=\u001b[39m OpenAI(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopenai_config)\n\u001b[0;32m    427\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clients\u001b[38;5;241m.\u001b[39mappend(OpenAIClient(client))\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logging_enabled():\n",
      "\u001b[1;31mTypeError\u001b[0m: OpenAI.__init__() got an unexpected keyword argument 'api_version'"
     ]
    }
   ],
   "source": [
    "import autogen\n",
    "from autogen import AssistantAgent, Agent, UserProxyAgent, ConversableAgent\n",
    "\"\"\"\n",
    "20240219，此處列表更新為5類\n",
    "gpt35、gpt4、gpt4v、gemini、gemini_vision\n",
    "\"\"\"\n",
    "config_list_gpt35 = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-3.5-turbo\", \"gpt-3.5-turbo-1106\", \"gpt-3.5-turbo-0613\", \"gpt-3.5-turbo-16k\", \"gpt-3.5-turbo-16k-0613\"],\n",
    "    },\n",
    ")\n",
    "config_list_gpt4 = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4\", \"gpt-4-0613\", \"gpt-4-0314\", \"gpt-4-1106-preview\"],\n",
    "    },\n",
    ")\n",
    "config_list_gpt4v = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4-vision-preview\", \"dalle\"],\n",
    "    },\n",
    ")\n",
    "config_list_gemini = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gemini-pro\"],\n",
    "    },\n",
    ")\n",
    "config_list_gemini_vision = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gemini-pro-vision\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-3.5-turbo-1106\"], # gpt-3.5-turbo-16k, gpt-3.5-turbo, gpt-3.5-turbo-1106\n",
    "    },\n",
    ")\n",
    "llm_config={\"config_list\": config_list, \"seed\": 42}\n",
    "\n",
    "# 定義2個Agent，assistant和user_proxy\n",
    "# 但不清楚為何跟下面區塊直接設定使用哪個模型不一樣，這裡用 \"assistant\" 跟 \"user_proxy\"\n",
    "# 下面的區塊是用 \"gpt-3.5-turbo-1106\"、\"gemini-pro\",\n",
    "assistant = AssistantAgent(\n",
    "    \"assistant\", llm_config={\"config_list\": config_list_gpt35, \"seed\": 42}, max_consecutive_auto_reply=3\n",
    ")\n",
    "# print(assistant.system_message)\n",
    "# 這個 user_proxy 模型用 \"user_proxy\" ??? 不懂\n",
    "# 也沒有 llm_config，但還是可以執行???\n",
    "# UserProxyAgent 應該只能使用 openAI，其他會報錯，但這個區塊卻又可以????? 不過，整個來說，幾乎都是不行的。\n",
    "user_proxy = UserProxyAgent(\n",
    "    \"user_proxy\",\n",
    "    code_execution_config={\"work_dir\": \"coding\", \"use_docker\": False},\n",
    "    human_input_mode=\"NEVER\",\n",
    "    llm_config={\"config_list\": config_list_gpt35, \"seed\": 42},\n",
    "    is_termination_msg=lambda x: content_str(x.get(\"content\")).find(\"TERMINATE\") >= 0,\n",
    ")\n",
    "# 由 user_proxy開始發起對話(initiate_chat)\n",
    "user_proxy.initiate_chat(assistant, message=\"請搜尋arXiv上有關醫學領域蛋白質研究最前緣的技術有哪些\")  # 這個會報錯\n",
    "# user_proxy.initiate_chat(assistant, message=\"Sort the array with Bubble Sort: [4, 1, 5, 2]\")     # 這個卻可以 ????\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 增加一個 agent\n",
    "# define a custom agent\n",
    "pm = autogen.AssistantAgent(\"Product_manager\", system_message=\"Creative in software product ideas.\", llm_config=llm_config)\n",
    "\n",
    "groupchat = autogen.GroupChat(agents=[user_proxy, coder, pm], messages=[])\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config, human_input_mode=\"NEVER\")\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\"user_proxy\", \n",
    "                                    human_input_mode=\"NEVER\",\n",
    "                                    code_execution_config={\"work_dir\": \"coding\", \"use_docker\": False})\n",
    "coder = autogen.AssistantAgent(\"coder\", llm_config=llm_config)\n",
    "\n",
    "# task = \"Find a lastest paper about gpt-4 on arxiv and and find its potential applications in software. Final result should reply in traditional chinese.\"\n",
    "task = \"Find a lastest paper about gpt-4 on arxiv and and find its potential applications in software.\"\n",
    "user_proxy.initiate_chat(manager, message = task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "from autogen import AssistantAgent, UserProxyAgent\n",
    "config_list = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-3.5-turbo-1106\"],\n",
    "    },\n",
    ")\n",
    "llm_config={\"config_list\": config_list, \"seed\": 42}\n",
    "\n",
    "# create a UserProxyAgent instance named \"user_proxy\"\n",
    "user_proxy = autogen.UserProxyAgent(\"user_proxy\", \n",
    "                                    human_input_mode=\"NEVER\",\n",
    "                                    code_execution_config={\"work_dir\": \"coding\", \"use_docker\": False})\n",
    "\n",
    "# create an AssistantAgent instance named \"assistant\"｝不指定 llm_config 會出現警告訊息\n",
    "assistant = AssistantAgent(name=\"assistant\", llm_config=llm_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
