{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67cee7e3",
   "metadata": {},
   "source": [
    "# Using Gemini in AutoGen with Other LLMs\n",
    "\n",
    "user_proxy 使用 gemini pro 會報錯\n",
    "\n",
    "You don't need to handle OpenAI or Google's GenAI packages. AutoGen already handled all of these for you.\n",
    "\n",
    "You can just create different agents with different backend LLM with assistant agent, and all models/agents are at your fingertip.\n",
    "\n",
    "\n",
    "## Main Distinctions\n",
    "- Gemini does not have the \"system_message\" field (correct me if I am wrong). So, it's instruction following skills are not as strong as GPTs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d801d13b",
   "metadata": {},
   "source": [
    "Sample OAI_CONFIG_LIST \n",
    "\n",
    "```python\n",
    "[\n",
    "    {\n",
    "        \"model\": \"gpt-35-turbo\",\n",
    "        \"api_key\": \"your OpenAI Key goes here\",\n",
    "        \"base_url\": \"https://tnrllmproxy.azurewebsites.net/v1\",\n",
    "        \"api_version\": \"2023-06-01-preview\"\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"gpt-4-vision-preview\",\n",
    "        \"api_key\": \"your OpenAI Key goes here\",\n",
    "        \"api_version\": \"2023-06-01-preview\"\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"dalle\",\n",
    "        \"api_key\": \"your OpenAI Key goes here\",\n",
    "        \"api_version\": \"2023-06-01-preview\"\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"gemini-pro\",\n",
    "        \"api_key\": \"your Google's GenAI Key goes here\",\n",
    "        \"api_type\": \"google\"\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"gemini-pro-vision\",\n",
    "        \"api_key\": \"your Google's GenAI Key goes here\",\n",
    "        \"api_type\": \"google\"\n",
    "    }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fee066",
   "metadata": {},
   "source": [
    "### Before everything starts, install AutoGen with the `gemini` option\n",
    "```bash\n",
    "pip install \"pyautogen[gemini]~=0.2.0b4\"\n",
    "```\n",
    "\n",
    "\n",
    "#### Install These Missing Packages Manually if You Encounter Any Errors\n",
    "```bash\n",
    "pip install https://github.com/microsoft/autogen/archive/gemini.zip\n",
    "pip install \"google-generativeai\" \"pydash\" \"pillow\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37cd165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install https://github.com/microsoft/autogen/archive/gemini.zip\n",
    "# !pip install \"google-generativeai\" \"pydash\" \"pillow\"\n",
    "import requests\n",
    "import json\n",
    "import pdb\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union\n",
    "import autogen\n",
    "from autogen import AssistantAgent, Agent, UserProxyAgent, ConversableAgent\n",
    "\n",
    "\n",
    "from autogen.agentchat.contrib.multimodal_conversable_agent import MultimodalConversableAgent\n",
    "from autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgent\n",
    "from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent\n",
    "from autogen.agentchat.contrib.img_utils import get_image_data, _to_pil\n",
    "from autogen.code_utils import DEFAULT_MODEL, UNKNOWN, content_str, execute_code, extract_code, infer_lang\n",
    "\n",
    "import chromadb\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from termcolor import colored\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed6b642",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "20240219，此處列表更新為5類\n",
    "gpt35、gpt4、gpt4v、gemini、gemini_vision\n",
    "\"\"\"\n",
    "config_list_gpt35 = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-3.5-turbo\", \"gpt-3.5-turbo-1106\", \"gpt-3.5-turbo-0613\", \"gpt-3.5-turbo-16k\", \"gpt-3.5-turbo-16k-0613\"],\n",
    "    },\n",
    ")\n",
    "config_list_gpt4 = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4\", \"gpt-4-0613\", \"gpt-4-0314\", \"gpt-4-1106-preview\"],\n",
    "    },\n",
    ")\n",
    "config_list_gpt4v = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4-vision-preview\", \"dalle\"],\n",
    "    },\n",
    ")\n",
    "config_list_gemini = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gemini-pro\"],\n",
    "    },\n",
    ")\n",
    "config_list_gemini_vision = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gemini-pro-vision\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c3221e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 此區塊命令用途，詢問 GPT\n",
    "\"\"\"\n",
    "這段代碼是使用列表推導式對一系列配置字典中的鍵 \"api_version\" 進行移除操作。\n",
    "讓我們來分解這段代碼：\n",
    "[config.pop(\"api_version\", None) for config in config_list_gpt4]：這行代碼對名為 config_list_gpt4 的列表中的每個配置字典 config 都執行了一個操作，即移除其中的 \"api_version\" 鍵。如果 \"api_version\" 鍵不存在於字典中，則不執行任何操作，這就是 pop 方法的作用。列表推導式的結果不會被使用，所以在這個例子中，主要是為了執行 pop 操作。\n",
    "其餘的行 (config_list_gpt4v, config_list_gpt35, config_list_gemini, config_list_gemini_vision) 也是類似的，只是對應不同的配置列表。\n",
    "總體來說，這段代碼的作用是從一系列配置字典中移除名為 \"api_version\" 的鍵，這可能是為了在後續的程式中不再使用該配置參數。\n",
    "\"\"\"\n",
    "[config.pop(\"api_version\", None) for config in config_list_gpt4]\n",
    "[config.pop(\"api_version\", None) for config in config_list_gpt4v]\n",
    "[config.pop(\"api_version\", None) for config in config_list_gpt35]\n",
    "[config.pop(\"api_version\", None) for config in config_list_gemini]\n",
    "[config.pop(\"api_version\", None) for config in config_list_gemini_vision]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f09481",
   "metadata": {},
   "source": [
    "## Gemini Assitant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c6e552",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 定義2個Agent，assistant和user_proxy\n",
    "# 但不清楚為何跟下面區塊直接設定使用哪個模型不一樣，這裡用 \"assistant\" 跟 \"user_proxy\"\n",
    "# 下面的區塊是用 \"gpt-3.5-turbo-1106\"、\"gemini-pro\",\n",
    "assistant = AssistantAgent(\n",
    "    \"assistant\", llm_config={\"config_list\": config_list_gemini, \"seed\": 42}, max_consecutive_auto_reply=3\n",
    ")\n",
    "# print(assistant.system_message)\n",
    "# 這個 user_proxy 模型用 \"user_proxy\" ??? 不懂\n",
    "# 也沒有 llm_config，但還是可以執行???\n",
    "# UserProxyAgent 應該只能使用 openAI，其他會報錯，但這個區塊卻又可以????? 不過，整個來說，幾乎都是不行的。\n",
    "user_proxy = UserProxyAgent(\n",
    "    \"user_proxy\",\n",
    "    code_execution_config={\"work_dir\": \"coding\", \"use_docker\": False},\n",
    "    human_input_mode=\"NEVER\",\n",
    "    # llm_config={\"config_list\": config_list_gemini, \"seed\": 42},\n",
    "    is_termination_msg=lambda x: content_str(x.get(\"content\")).find(\"TERMINATE\") >= 0,\n",
    ")\n",
    "# 由 user_proxy開始發起對話(initiate_chat)\n",
    "user_proxy.initiate_chat(assistant, message=\"請搜尋arXiv上有關醫學領域蛋白質研究最前緣的技術有哪些\")  # 這個會報錯\n",
    "# user_proxy.initiate_chat(assistant, message=\"Sort the array with Bubble Sort: [4, 1, 5, 2]\")     # 這個卻可以 ????"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de85984",
   "metadata": {},
   "source": [
    "## Agent Collaboration and Interactions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2f2bb5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 定義2個 agent(gpt、 gemini)，其中 gpt 是用 gpt-3.5-turbo-1106，gemini 是用 gemini-pro。\n",
    "gpt = AssistantAgent(\n",
    "    # \"GPT-4\",\n",
    "    \"gpt-3.5-turbo-1106\",\n",
    "    system_message=\"\"\"You should ask weird, tricky, and concise questions.\n",
    "Ask the next question based on (by evolving) the previous one. You should always reply in traditional chinese.\"\"\",\n",
    "    llm_config={\"config_list\": config_list_gpt35, \"seed\": 42},\n",
    "    max_consecutive_auto_reply=3,\n",
    ")\n",
    "\n",
    "# gemini 可以用，但很容易安全審查不通過導致無法對話後，就一段時間不回應\n",
    "gemini = AssistantAgent(\n",
    "    \"gemini-pro\",\n",
    "    system_message=\"\"\"Always answer questions within one sentence. You should always reply in traditional chinese.\"\"\",\n",
    "    #                      system_message=\"answer:\",\n",
    "    llm_config={\"config_list\": config_list_gemini, \"seed\": 42},\n",
    "    max_consecutive_auto_reply=4,\n",
    ")\n",
    "# 由 gpt 對 gemini 先發起對話，\"變形金剛購買汽車保險或健康保險嗎？'\n",
    "gpt.initiate_chat(gemini, message=\"變形金剛應該購買汽車保險或健康保險？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c079aa7",
   "metadata": {},
   "source": [
    "Let's switch position. Now, Gemini is the question raiser. \n",
    "\n",
    "This time, Gemini could not follow the system instruction well or evolve questions, because the Gemini does not handle system messages similar to GPTs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1ead90",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gpt = AssistantAgent(\n",
    "    \"gpt-3.5-turbo-1106\",\n",
    "    system_message=\"\"\"Always answer questions within one sentence. You should always reply in traditional chinese.\"\"\",\n",
    "    llm_config={\"config_list\": config_list_gpt35, \"seed\": 42},\n",
    "    max_consecutive_auto_reply=3,\n",
    ")\n",
    "# 這裡的 gemini 可以使用\n",
    "gemini = AssistantAgent(\n",
    "    \"gemini-pro\",\n",
    "    system_message=\"\"\"You should ask weird, tricky, and concise questions.\n",
    "Ask the next question based on (by evolving) the previous one. You should always reply in traditional chinese.\"\"\",\n",
    "    llm_config={\"config_list\": config_list_gemini, \"seed\": 42},\n",
    "    max_consecutive_auto_reply=4,\n",
    ")\n",
    "\n",
    "gemini.initiate_chat(gpt, message=\"Should Spider Man invest in 401K?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bf9c11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c35635e3",
   "metadata": {},
   "source": [
    "## Gemini RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f66022",
   "metadata": {},
   "source": [
    "Here we will be exploring RAG with Gemini. Note that Gemini will raise a 500 error if a message is an empty string. To prevent this, we set the `default_auto_reply` to `Reply plaintext TERMINATE to exit.` for the `ragproxyagent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd1ea1c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1. create an RetrieveAssistantAgent instance named \"assistant\"\n",
    "assistant = RetrieveAssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    system_message=\"You are a helpful assistant.\",\n",
    "    llm_config={\n",
    "        \"timeout\": 600,\n",
    "        \"cache_seed\": 42,\n",
    "        \"config_list\": config_list_gemini,\n",
    "    },\n",
    ")\n",
    "\n",
    "# 2. create the RetrieveUserProxyAgent instance named \"ragproxyagent\"\n",
    "ragproxyagent = RetrieveUserProxyAgent(\n",
    "    name=\"ragproxyagent\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    default_auto_reply=\"Reply plaintext TERMINATE to exit.\",  # Gemini will raise 500 error if the response is empty.\n",
    "    max_consecutive_auto_reply=3,\n",
    "    retrieve_config={\n",
    "        \"task\": \"code\",\n",
    "        \"docs_path\": [\n",
    "            \"https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Examples/Integrate%20-%20Spark.md\",\n",
    "            \"https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Research.md\",\n",
    "            os.path.join(os.path.abspath(\"\"), \"..\", \"website\", \"docs\"),\n",
    "        ],\n",
    "        \"custom_text_types\": [\"mdx\"],\n",
    "        \"chunk_token_size\": 2000,\n",
    "        \"model\": config_list_gemini[0][\"model\"],\n",
    "        \"client\": chromadb.PersistentClient(path=\"/tmp/chromadb\"),\n",
    "        \"embedding_model\": \"all-mpnet-base-v2\",\n",
    "        \"get_or_create\": True,  # set to False if you don't want to reuse an existing collection, but you'll need to remove the collection manually\n",
    "    },\n",
    "    code_execution_config=False,  # set to False if you don't want to execute the code\n",
    ")\n",
    "\n",
    "code_problem = \"How can I use FLAML to perform a classification task and use spark to do parallel training. Train 60 seconds and force cancel jobs if time limit is reached.\"\n",
    "ragproxyagent.initiate_chat(\n",
    "    assistant, problem=code_problem, search_string=\"spark\"\n",
    ")  # search_string is used as an extra filter for the embeddings search, in this case, we only want to search documents that contain \"spark\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09139b42",
   "metadata": {},
   "source": [
    "## Gemini Multimodal\n",
    "\n",
    "You can create multimodal agent for Gemini the same way as the GPT-4V and LLaVA.\n",
    "\n",
    "\n",
    "Note that the Gemini-pro-vision does not support chat yet. So, we only use the last message in the prompt for multi-turn chat. The behavior might be strange compared to GPT-4V and LLaVA models.\n",
    "\n",
    "Here, we ask a question about \n",
    "![](https://github.com/microsoft/autogen/blob/main/website/static/img/chat_example.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5098e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "image_agent = MultimodalConversableAgent(\n",
    "    \"Gemini Vision\", llm_config={\"config_list\": config_list_gemini_vision, \"seed\": 42}, max_consecutive_auto_reply=1\n",
    ")\n",
    "\n",
    "user_proxy = UserProxyAgent(\"user_proxy\", human_input_mode=\"NEVER\", max_consecutive_auto_reply=0)\n",
    "\n",
    "# user_proxy.initiate_chat(image_agent,\n",
    "#                          message=\"\"\"What's the breed of this dog?\n",
    "# <img https://th.bing.com/th/id/R.422068ce8af4e15b0634fe2540adea7a?rik=y4OcXBE%2fqutDOw&pid=ImgRaw&r=0>.\"\"\")\n",
    "\n",
    "user_proxy.initiate_chat(\n",
    "    image_agent,\n",
    "    message=\"\"\"What is this image about?\n",
    "<img https://github.com/microsoft/autogen/blob/main/website/static/img/chat_example.png?raw=true>.\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0cfaf7",
   "metadata": {},
   "source": [
    "## GroupChat with Gemini Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0d7ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1 = AssistantAgent(\n",
    "    \"Gemini-agent\",\n",
    "    llm_config={\"config_list\": config_list_gemini, \"seed\": 42},\n",
    "    max_consecutive_auto_reply=3,\n",
    "    system_message=\"Answer questions about Google.\",\n",
    "    description=\"I am good at answering questions about Google and Research papers.\",\n",
    ")\n",
    "\n",
    "agent2 = AssistantAgent(\n",
    "    \"GPT-agent\",\n",
    "    llm_config={\"config_list\": config_list_gpt4, \"seed\": 42},\n",
    "    max_consecutive_auto_reply=3,\n",
    "    description=\"I am good at writing code.\",\n",
    ")\n",
    "\n",
    "user_proxy = UserProxyAgent(\n",
    "    \"user_proxy\",\n",
    "    code_execution_config={\"work_dir\": \"coding\", \"use_docker\": False},\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=5,\n",
    "    is_termination_msg=lambda x: content_str(x.get(\"content\")).find(\"TERMINATE\") >= 0\n",
    "    or content_str(x.get(\"content\")) == \"\",\n",
    "    description=\"I stands for user, and can run code.\",\n",
    ")\n",
    "\n",
    "groupchat = autogen.GroupChat(agents=[agent1, agent2, user_proxy], messages=[], max_round=12)\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config={\"config_list\": config_list_gemini, \"seed\": 42})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c24344",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# user_proxy.initiate_chat(manager, message=\"Show me the release year of famous Google products.\")\n",
    "user_proxy.send(\"Show me the release year of famous Google products in a table.\", recipient=manager, request_reply=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5998cce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "user_proxy.send(\n",
    "    \"Plot the products and years in scatter plot and save to `graph.png`\", recipient=manager, request_reply=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b898fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img = Image.open(\"coding/graph.png\")\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33c837f",
   "metadata": {},
   "source": [
    "## A Larger Example of Group Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b532bc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "coder = AssistantAgent(\n",
    "    name=\"Coder\",\n",
    "    llm_config={\"config_list\": config_list_gemini, \"seed\": 42},\n",
    "    max_consecutive_auto_reply=10,\n",
    "    description=\"I am good at writing code\",\n",
    ")\n",
    "\n",
    "pm = AssistantAgent(\n",
    "    name=\"Product_manager\",\n",
    "    system_message=\"Creative in software product ideas.\",\n",
    "    llm_config={\"config_list\": config_list_gemini, \"seed\": 42},\n",
    "    max_consecutive_auto_reply=10,\n",
    "    description=\"I am good at design products and software.\",\n",
    ")\n",
    "\n",
    "user_proxy = UserProxyAgent(\n",
    "    name=\"User_proxy\",\n",
    "    code_execution_config={\"last_n_messages\": 20, \"work_dir\": \"coding\", \"use_docker\": False},\n",
    "    human_input_mode=\"TERMINATE\",\n",
    "    is_termination_msg=lambda x: content_str(x.get(\"content\")).find(\"TERMINATE\") >= 0,\n",
    "    description=\"I stands for user, and can run code.\",\n",
    ")\n",
    "\n",
    "groupchat = autogen.GroupChat(agents=[user_proxy, coder, pm], messages=[], max_round=12)\n",
    "manager = autogen.GroupChatManager(\n",
    "    groupchat=groupchat,\n",
    "    llm_config={\"config_list\": config_list_gemini, \"seed\": 42},\n",
    "    is_termination_msg=lambda x: content_str(x.get(\"content\")).find(\"TERMINATE\") >= 0,\n",
    ")\n",
    "user_proxy.initiate_chat(\n",
    "    manager,\n",
    "    message=\"\"\"Design and implement a multimodal product for people with vision disabilities.\n",
    "The pipeline will take an image and run Gemini model to describe:\n",
    "1. what objects are in the image, and\n",
    "2. where these objects are located.\"\"\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdec308",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53b56419",
   "metadata": {},
   "source": [
    "## GPT  v.s. Gemini Arena\n",
    "\n",
    "Do you remember AutoGen can ask LLMs to play chess. Here, we create an arena to allow GPT and Gemini fight together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b18ca19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e47c9c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
