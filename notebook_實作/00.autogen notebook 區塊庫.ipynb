{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Examples 有提供各案例及 notebook\n",
    "https://microsoft.github.io/autogen/docs/Examples\n",
    "https://microsoft.github.io/autogen/\n",
    "https://microsoft.github.io/autogen/docs/Use-Cases/enhanced_inference/#api-unification\n",
    "https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat/\n",
    "https://microsoft.github.io/autogen/docs/llm_configuration\n",
    "https://github.com/microsoft/autogen/blob/main/notebook/config_loader_utility_functions.ipynb\n",
    "https://microsoft.github.io/autogen/blog/\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "from autogen.cache import Cache\n",
    "from autogen import AssistantAgent, UserProxyAgent, config_list_from_json\n",
    "\n",
    "# Load LLM inference endpoints from an environment variable or a file\n",
    "config_list = config_list_from_json(env_or_file=\"OAI_CONFIG_LIST\")\n",
    "\"\"\"\n",
    "20240219，此處列表更新為5類\n",
    "gpt35、gpt4、gpt4v、\n",
    "gemini、gemini_vision(gemini 還不大行，會報錯，暫不支援)\n",
    "cohere 可以調用，但免費的很快超過額度\n",
    "groq   可以調用，但有點怪怪的。\n",
    "也加入本地大模型，但相容性似乎都有問題，第一輪對話還可以，第一輪之後都不行。\n",
    "- ollama：要使用 litellm\n",
    "- LM Studio \n",
    "- Jan (2024/03/02 開啟GPU加速會報錯)\n",
    "\"\"\"\n",
    "config_list_gpt35 = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-3.5-turbo\", \"gpt-3.5-turbo-1106\", \"gpt-3.5-turbo-0613\", \"gpt-3.5-turbo-16k\", \"gpt-3.5-turbo-16k-0613\"],\n",
    "    },\n",
    ")\n",
    "config_list_ollama = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"ollama\"],\n",
    "    },\n",
    ")\n",
    "config_list_LM_Satudio = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"LM Studio\"],\n",
    "    },\n",
    ")\n",
    "config_list_jan = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"OpenHermes Neural 7B Q4\"],\n",
    "    },\n",
    ")\n",
    "config_list_groq_llama2 = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"llama2-70b-4096\"],\n",
    "    },\n",
    ")\n",
    "config_list_groq_mixtral = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"mixtral-8x7b-32768\"],\n",
    "    },\n",
    ")\n",
    "config_list_cohere = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"command-nightly\"],\n",
    "    },\n",
    ")\n",
    "config_list_gpt4 = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4\", \"gpt-4-0613\", \"gpt-4-0314\", \"gpt-4-1106-preview\"],\n",
    "    },\n",
    ")\n",
    "config_list_gpt4v = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4-vision-preview\", \"dalle\"],\n",
    "    },\n",
    ")\n",
    "config_list_gemini = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gemini-pro\"],\n",
    "    },\n",
    ")\n",
    "config_list_gemini_vision = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gemini-pro-vision\"],\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 這裡設定要用那一個大模型，試吧，GPT 的幾乎沒遇過問題，不是 GPT 的，永遠都是在遇到問題\n",
    "# config_list = config_list_groq_llama2\n",
    "config_list = config_list_ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = AssistantAgent(\"assistant\", llm_config={\"config_list\": config_list},max_consecutive_auto_reply=4)\n",
    "user_proxy = UserProxyAgent(\"user_proxy\", code_execution_config={\"work_dir\": \"coding\", \"use_docker\": False}, llm_config={\"config_list\": config_list}, \n",
    "                            human_input_mode=\"NEVER\",max_consecutive_auto_reply=4\n",
    "                            )\n",
    "\"\"\"\n",
    "# 這裡使用 Chche 的目的?\n",
    "with Cache.disk() as cache:\n",
    "    # start the conversation\n",
    "    user_proxy.initiate_chat(\n",
    "        assistant,\n",
    "        message=\"Draw two agents chatting with each other with an example dialog.\",\n",
    "        # message=\"Draw two agents chatting with each other with an example dialog. Don't add plt.show().\",\n",
    "        cache=cache,\n",
    "    )\n",
    "\"\"\"\n",
    "# Kickstart a conversation between the agents to plot a stock price chart\n",
    "user_proxy.initiate_chat(assistant, message=\"Plot a chart of NVDA and TESLA stock price change YTD.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "from IPython import get_ipython\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "# 限定只用 gpt3.5\n",
    "config_list = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"ollama\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "from IPython.display import HTML, display, Markdown\n",
    "\n",
    "def to_markdown(text):\n",
    "  text = text.replace('•', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_proxy 負責執行程式碼 或是 獲取人類的輸入\n",
    "user_proxy=autogen.UserProxyAgent(\n",
    "        name=\"user_proxy\",\n",
    "        human_input_mode=\"NEVER\",\n",
    "    \tcode_execution_config={\"work_dir\": \"coding\", \"use_docker\": False},\n",
    "        is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "        )\n",
    "# \n",
    "assistant=autogen.AssistantAgent(\n",
    "        name=\"assistant\",\n",
    "\tsystem_message=\"You are a helpful assistant. Reply TERMINATE when the task is done.\",\n",
    "        llm_config={\"config_list\":config_list, \n",
    "                    # \"seed\": 42,\n",
    "                    }, \n",
    "\tmax_consecutive_auto_reply=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定任務 task\n",
    "task = \"Solve x^2 + 3x + 2 = 0, and explained the solution in Traditional Chinese.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 開始對話解題\n",
    "user_proxy.initiate_chat(assistant,message=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用\"function call\"來實現向專家提問的拓展。但下一個區塊會報錯\"Error: Function ask_expert not found.\"\n",
    "# 修正方式如下：\n",
    "\"\"\"\n",
    "# 會用到 Annotated，先匯入\n",
    "from typing_extensions import Annotated\n",
    "# 註冊函數跟定義函式的格式\n",
    "@user_proxy.register_for_execution()\n",
    "@assistant.register_for_llm(name=\"expert\", description=\"expert to solve math question.\")\n",
    "def expert(message: Annotated[str, \"Math question to solve.\"]) ->str:\n",
    "\"\"\"\n",
    "@user_proxy.register_for_execution()\n",
    "@assistant.register_for_llm(name=\"expert\", description=\"expert to solve math question.\")\n",
    "def expert(message: Annotated[str, \"Math question to solve.\"]) ->str:\n",
    "\tassistant_for_expert = autogen.AssistantAgent(\n",
    "\t\tname = \"assistant_for_expert\",\n",
    "\t\tllm_config = {\n",
    "\t\t\t\"temperature\":1,\n",
    "\t\t\t\"config_list\":config_list\n",
    "\t\t\t}\n",
    "\t\t)\n",
    "\texpert = autogen.UserProxyAgent(\n",
    "\t\tname = \"expert\",\n",
    "\t\thuman_input_mode = \"NEVER\",  # ALWAYS會一直要求輸入，不知道要輸入什麼，不然就改成 NEVER\n",
    "\t\tcode_execution_config = {\"work_dir\":\"expert\", \"use_docker\": False}\n",
    "\t\t)\n",
    "\texpert.initiate_chat(assistant_for_expert, message = message)\n",
    "\texpert.stop_reply_at_receive(assistant_for_expert)\n",
    "\texpert.send(\"解出答案並以淺顯易懂的方式解釋怎麼解題\", assistant_for_expert)\n",
    "\treturn expert.last_message()[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions according to the function description\n",
    "# 應該不需要 exec_python 這個 tools? user_proxy 不是已經有 code_execution_config={\"work_dir\": \"coding\", \"use_docker\": False}, ？？\n",
    "# 第一種方法:使用 register_for_execution 及 register_for_llm 二個裝飾器來註冊函數\n",
    "\n",
    "@user_proxy.register_for_execution()\n",
    "@assistant.register_for_llm(name=\"python\", description=\"run cell in ipython and return the execution result.\")\n",
    "def exec_python(cell: Annotated[str, \"Valid Python cell to execute.\"]) -> str:\n",
    "    ipython = get_ipython()\n",
    "    result = ipython.run_cell(cell)\n",
    "    log = str(result.result)\n",
    "    if result.error_before_exec is not None:\n",
    "        log += f\"\\n{result.error_before_exec}\"\n",
    "    if result.error_in_exec is not None:\n",
    "        log += f\"\\n{result.error_in_exec}\"\n",
    "    return log\n",
    "\n",
    "# 第二種方法:使用 register_function 來註冊函數\n",
    "def exec_sh(script: Annotated[str, \"Valid Python cell to execute.\"]) -> str:\n",
    "    return user_proxy.execute_code_blocks([(\"sh\", script)])\n",
    "\n",
    "autogen.agentchat.register_function(\n",
    "    exec_python,\n",
    "    caller=assistant,\n",
    "    executor=user_proxy,\n",
    "    name=\"sh\",\n",
    "    description=\"run a shell script and return the execution result.\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定任務 task\n",
    "task = \"Solve x^2 + 3x + 2 = 0, and explained the solution in Traditional Chinese.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 開始對話解題\n",
    "user_proxy.initiate_chat(assistant,message=task)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
