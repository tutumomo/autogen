{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1004af6a7fbfcd8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# AutoBuild 自動建構\n",
    "AutoGen 提供由 LLM、工具或人員支援的可對話代理，可用於透過自動聊天集體執行任務。 該框架允許透過多代理對話使用工具和人類參與。\n",
    "請在[此處](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat)尋找有關此功能的文件。\n",
    "\n",
    "在本筆記本中，我們引入了一個新類別“AgentBuilder”，以幫助使用者建立由多代理系統支援的自動任務解決流程。 具體來說，在`build()`中，我們提示LLM建立多個參與者代理程式並初始化群組聊天，並指定該任務是否需要編程來解決。 AgentBuilder 也透過 [vLLM](https://docs.vllm.ai/en/latest/index.html) 和 [Fastchat](https://github.com/lm-sys/FastChat) 支援開源 LLM。 檢查支援的模型清單[此處](https://docs.vllm.ai/en/latest/models/supported_models.html)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec78dda8e3826d8a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Requirement 要求\n",
    "\n",
    "AutoBuild 需要最新版本的 AutoGen。\n",
    "您可以透過以下命令安裝 AutoGen："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e9ae50658be975",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pip install -U --quiet pyautogen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "973213a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import autogen\n",
    "from autogen import AssistantAgent, Agent, UserProxyAgent, ConversableAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0e63ab3604bdb9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 1: prepare configuration 第 1 步：準備配置\n",
    "為助理代理準備一個“config_path”，以限制您要在此任務中使用的LLM的選擇。 此配置可以是json檔案的路徑或環境變數的名稱。 初始化 LLM 的特定配置（如種子、溫度等）也需要「default_llm_config」..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2505f029423b21ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T13:31:44.147211100Z",
     "start_time": "2023-12-03T13:31:44.121842300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config_path = 'OAI_CONFIG_LIST'  # modify path\n",
    "default_llm_config = {\n",
    "    'temperature': 0\n",
    "}\n",
    "\"\"\"\n",
    "20240219，此處列表更新為5類\n",
    "gpt35、gpt4、gpt4v、\n",
    "gemini、gemini_vision(gemini 還不大行，會報錯，暫不支援)\n",
    "cohere 可以調用，但免費的很快超過額度\n",
    "groq   可以調用，但有點怪怪的。\n",
    "也加入本地大模型，但相容性似乎都有問題，第一輪對話還可以，第一輪之後都不行。\n",
    "- ollama：要使用 litellm\n",
    "- LM Studio \n",
    "- Jan (2024/03/02 開啟GPU加速會報錯)\n",
    "\"\"\"\n",
    "config_list_gpt35 = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-3.5-turbo\", \"gpt-3.5-turbo-1106\", \"gpt-3.5-turbo-0613\", \"gpt-3.5-turbo-16k\", \"gpt-3.5-turbo-16k-0613\"],\n",
    "    },\n",
    ")\n",
    "config_list_ollama = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"ollama\"],\n",
    "    },\n",
    ")\n",
    "config_list_LM_Satudio = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"LM Studio\"],\n",
    "    },\n",
    ")\n",
    "config_list_jan = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"OpenHermes Neural 7B Q4\"],\n",
    "    },\n",
    ")\n",
    "config_list_groq_llama2 = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"llama2-70b-4096\"],\n",
    "    },\n",
    ")\n",
    "config_list_groq_mixtral = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"mixtral-8x7b-32768\"],\n",
    "    },\n",
    ")\n",
    "config_list_cohere = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"command-nightly\"],\n",
    "    },\n",
    ")\n",
    "config_list_gpt4 = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4\", \"gpt-4-0613\", \"gpt-4-0314\", \"gpt-4-1106-preview\"],\n",
    "    },\n",
    ")\n",
    "config_list_gpt4v = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4-vision-preview\", \"dalle\"],\n",
    "    },\n",
    ")\n",
    "config_list_gemini = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gemini-pro\"],\n",
    "    },\n",
    ")\n",
    "config_list_gemini_vision = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gemini-pro-vision\"],\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11f1f7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 這裡設定要用那一個大模型\n",
    "config_list = config_list_groq_mixtral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d6586c68fa425b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 2: create a AgentBuilder 第二步：建立AgentBuilder\n",
    "使用指定的“config_path”建立“AgentBuilder”。 AgentBuilder預設使用GPT-4來完成整個過程，您也可以根據需要將“builder_model”更改為其他OpenAI模型。 您也可以指定 OpenAI 或開源 LLM 作為代理主幹，請參閱部落格以了解更多詳細資訊。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfa67c771a0fed37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T13:31:44.996307300Z",
     "start_time": "2023-12-03T13:31:44.743284700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from autogen.agentchat.contrib.agent_builder import AgentBuilder\n",
    "\n",
    "# builder = AgentBuilder(config_path=config_path, builder_model='gpt-3.5-turbo-16k', agent_model='gpt-3.5-turbo-16k')\n",
    "# builder = AgentBuilder(builder_model='gpt-3.5-turbo-16k', agent_model='gpt-3.5-turbo-16k')\n",
    "# builder = AgentBuilder(builder_model='gemini-pro', agent_model='gemini-pro')\n",
    "# builder = AgentBuilder(builder_model='ollama', agent_model='ollama')\n",
    "# builder = AgentBuilder(builder_model='LM Studio', agent_model='LM Studio')\n",
    "# builder = AgentBuilder(builder_model='command-nightly', agent_model='command-nightly')  # config_list = config_list_cohere\n",
    "# builder = AgentBuilder(builder_model='llama2-70b-4096', agent_model='llama2-70b-4096') # config_list_groq_llama2\n",
    "builder = AgentBuilder(builder_model='mixtral-8x7b-32768', agent_model='mixtral-8x7b-32768') # config_list_groq_mixtral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6a655fb6618324",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 3: specify a building task\n",
    "\n",
    "Specify a building task with a general description. Building task will help build manager (a LLM) decide what agents should be build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68315f6ec912c58a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T13:31:45.444044500Z",
     "start_time": "2023-12-03T13:31:45.429483200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "building_task = \"Find a paper on arxiv by programming, and analyze its application in some domain. For example, find a recent paper about gpt-4 on arxiv and find its potential applications in software.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5782dd5ecb6c217a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 4: build group chat agents\n",
    "Use `build()` to let build manager (the specified `builder_model`) complete the group chat agents generation. If you think coding is necessary in your task, you can use `coding=True` to add a user proxy (an automatic code interpreter) into the agent list, like: \n",
    "```python\n",
    "builder.build(building_task, default_llm_config, coding=True)\n",
    "```\n",
    "If `coding` is not specified, AgentBuilder will determine on its own whether the user proxy should be added or not according to the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab490fdbe46c0473",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T13:32:45.887656900Z",
     "start_time": "2023-12-03T13:31:46.822373400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Fail to initialize build manager: mixtral-8x7b-32768 does not exist in OAI_CONFIG_LIST. If you want to change this model, please specify the \"builder_model\" in the constructor.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m default_llm_config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig_list\u001b[39m\u001b[38;5;124m\"\u001b[39m: config_list, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m42\u001b[39m}\n\u001b[1;32m----> 2\u001b[0m agent_list, agent_configs \u001b[38;5;241m=\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilding_task\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_llm_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# agent_list, agent_configs = builder.build(building_task, llm_config={\"config_list\": config_list_gemini, \"seed\": 42})\u001b[39;00m\n",
      "File \u001b[1;32md:\\TOMO.Project\\autogen\\venv\\Lib\\site-packages\\autogen\\agentchat\\contrib\\agent_builder.py:377\u001b[0m, in \u001b[0;36mAgentBuilder.build\u001b[1;34m(self, building_task, default_llm_config, coding, code_execution_config, use_oai_assistant, **kwargs)\u001b[0m\n\u001b[0;32m    371\u001b[0m config_list \u001b[38;5;241m=\u001b[39m autogen\u001b[38;5;241m.\u001b[39mconfig_list_from_json(\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_file_or_env,\n\u001b[0;32m    373\u001b[0m     file_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_file_location,\n\u001b[0;32m    374\u001b[0m     filter_dict\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder_model]},\n\u001b[0;32m    375\u001b[0m )\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(config_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 377\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    378\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFail to initialize build manager: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_file_or_env\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    379\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you want to change this model, please specify the \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuilder_model\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in the constructor.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    380\u001b[0m     )\n\u001b[0;32m    381\u001b[0m build_manager \u001b[38;5;241m=\u001b[39m autogen\u001b[38;5;241m.\u001b[39mOpenAIWrapper(config_list\u001b[38;5;241m=\u001b[39mconfig_list)\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m==> Generating agents...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Fail to initialize build manager: mixtral-8x7b-32768 does not exist in OAI_CONFIG_LIST. If you want to change this model, please specify the \"builder_model\" in the constructor."
     ]
    }
   ],
   "source": [
    "default_llm_config={\"config_list\": config_list, \"seed\": 42}\n",
    "agent_list, agent_configs = builder.build(building_task, default_llm_config)\n",
    "# agent_list, agent_configs = builder.build(building_task, llm_config={\"config_list\": config_list_gemini, \"seed\": 42})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00dd99880a4bf7b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 5: execute task\n",
    "Let agents generated in `build()` to complete the task collaboratively in a group chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d52e3d9a1bf91cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T13:33:42.369660600Z",
     "start_time": "2023-12-03T13:32:45.881740500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m1._Arxiv_Researcher\n",
      "2._Natural_Language_Processing_Expert\n",
      "3._Machine_Learning_Engineer\n",
      "4._Software_Engineer\n",
      "5._Domain_Analyst\u001b[0m (to chat_manager):\n",
      "\n",
      "Find a recent paper about gpt-4 on arxiv and find its potential applications in software.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "GroupChat is underpopulated with 1 agents. Please add more agents to the GroupChat or use direct communication instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 12\u001b[0m\n\u001b[0;32m      7\u001b[0m     manager \u001b[38;5;241m=\u001b[39m autogen\u001b[38;5;241m.\u001b[39mGroupChatManager(\n\u001b[0;32m      8\u001b[0m         groupchat\u001b[38;5;241m=\u001b[39mgroup_chat, llm_config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig_list\u001b[39m\u001b[38;5;124m\"\u001b[39m: config_list, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mllm_config}\n\u001b[0;32m      9\u001b[0m     )\n\u001b[0;32m     10\u001b[0m     agent_list[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39minitiate_chat(manager, message\u001b[38;5;241m=\u001b[39mexecution_task)\n\u001b[1;32m---> 12\u001b[0m \u001b[43mstart_task\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecution_task\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFind a recent paper about gpt-4 on arxiv and find its potential applications in software.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_llm_config\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 10\u001b[0m, in \u001b[0;36mstart_task\u001b[1;34m(execution_task, agent_list, llm_config)\u001b[0m\n\u001b[0;32m      6\u001b[0m group_chat \u001b[38;5;241m=\u001b[39m autogen\u001b[38;5;241m.\u001b[39mGroupChat(agents\u001b[38;5;241m=\u001b[39magent_list, messages\u001b[38;5;241m=\u001b[39m[], max_round\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m      7\u001b[0m manager \u001b[38;5;241m=\u001b[39m autogen\u001b[38;5;241m.\u001b[39mGroupChatManager(\n\u001b[0;32m      8\u001b[0m     groupchat\u001b[38;5;241m=\u001b[39mgroup_chat, llm_config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig_list\u001b[39m\u001b[38;5;124m\"\u001b[39m: config_list, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mllm_config}\n\u001b[0;32m      9\u001b[0m )\n\u001b[1;32m---> 10\u001b[0m \u001b[43magent_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitiate_chat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecution_task\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\TOMO.Project\\autogen\\venv\\Lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:913\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[1;34m(self, recipient, clear_history, silent, cache, max_turns, **context)\u001b[0m\n\u001b[0;32m    911\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    912\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_chat(recipient, clear_history)\n\u001b[1;32m--> 913\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_init_message\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    914\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_summarize_chat(\n\u001b[0;32m    915\u001b[0m     context\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, ConversableAgent\u001b[38;5;241m.\u001b[39mDEFAULT_summary_method),\n\u001b[0;32m    916\u001b[0m     recipient,\n\u001b[0;32m    917\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mcontext\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary_prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    918\u001b[0m     cache\u001b[38;5;241m=\u001b[39mcache,\n\u001b[0;32m    919\u001b[0m )\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28mself\u001b[39m, recipient]:\n",
      "File \u001b[1;32md:\\TOMO.Project\\autogen\\venv\\Lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:605\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[1;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[0;32m    603\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_oai_message(message, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, recipient)\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[1;32m--> 605\u001b[0m     \u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    607\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    608\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    609\u001b[0m     )\n",
      "File \u001b[1;32md:\\TOMO.Project\\autogen\\venv\\Lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:767\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[1;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreply_at_receive[sender] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    766\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 767\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_messages\u001b[49m\u001b[43m[\u001b[49m\u001b[43msender\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    769\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(reply, sender, silent\u001b[38;5;241m=\u001b[39msilent)\n",
      "File \u001b[1;32md:\\TOMO.Project\\autogen\\venv\\Lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:1740\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[1;34m(self, messages, sender, **kwargs)\u001b[0m\n\u001b[0;32m   1738\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1739\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger\u001b[39m\u001b[38;5;124m\"\u001b[39m], sender):\n\u001b[1;32m-> 1740\u001b[0m     final, reply \u001b[38;5;241m=\u001b[39m \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1741\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m final:\n\u001b[0;32m   1742\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m reply\n",
      "File \u001b[1;32md:\\TOMO.Project\\autogen\\venv\\Lib\\site-packages\\autogen\\agentchat\\groupchat.py:575\u001b[0m, in \u001b[0;36mGroupChatManager.run_chat\u001b[1;34m(self, messages, sender, config)\u001b[0m\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;66;03m# select the next speaker\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m     speaker \u001b[38;5;241m=\u001b[39m \u001b[43mgroupchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_speaker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspeaker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;66;03m# let the speaker speak\u001b[39;00m\n\u001b[0;32m    577\u001b[0m     reply \u001b[38;5;241m=\u001b[39m speaker\u001b[38;5;241m.\u001b[39mgenerate_reply(sender\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32md:\\TOMO.Project\\autogen\\venv\\Lib\\site-packages\\autogen\\agentchat\\groupchat.py:390\u001b[0m, in \u001b[0;36mGroupChat.select_speaker\u001b[1;34m(self, last_speaker, selector)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_speaker\u001b[39m(\u001b[38;5;28mself\u001b[39m, last_speaker: Agent, selector: ConversableAgent) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Agent:\n\u001b[0;32m    389\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Select the next speaker.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 390\u001b[0m     selected_agent, agents, messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_and_select_agents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlast_speaker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m selected_agent:\n\u001b[0;32m    392\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m selected_agent\n",
      "File \u001b[1;32md:\\TOMO.Project\\autogen\\venv\\Lib\\site-packages\\autogen\\agentchat\\groupchat.py:299\u001b[0m, in \u001b[0;36mGroupChat._prepare_and_select_agents\u001b[1;34m(self, last_speaker)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;66;03m# Warn if GroupChat is underpopulated\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_agents \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 299\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    300\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGroupChat is underpopulated with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_agents\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m agents. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease add more agents to the GroupChat or use direct communication instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    302\u001b[0m     )\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m n_agents \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeaker_selection_method\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mround_robin\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m allow_repeat_speaker:\n\u001b[0;32m    304\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGroupChat is underpopulated with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_agents\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m agents. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConsider setting speaker_selection_method to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mround_robin\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or allow_repeat_speaker to False, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor use direct communication, unless repeated speaker is desired.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: GroupChat is underpopulated with 1 agents. Please add more agents to the GroupChat or use direct communication instead."
     ]
    }
   ],
   "source": [
    "import autogen\n",
    "\n",
    "def start_task(execution_task: str, agent_list: list, llm_config: dict):\n",
    "    config_list = autogen.config_list_from_json(config_path, filter_dict={\"model\": [\"gpt-3.5-turbo-1106\"]})\n",
    "    \n",
    "    group_chat = autogen.GroupChat(agents=agent_list, messages=[], max_round=5)\n",
    "    manager = autogen.GroupChatManager(\n",
    "        groupchat=group_chat, llm_config={\"config_list\": config_list, **llm_config}\n",
    "    )\n",
    "    agent_list[0].initiate_chat(manager, message=execution_task)\n",
    "\n",
    "start_task(\n",
    "    execution_task=\"Find a recent paper about gpt-4 on arxiv and find its potential applications in software.\",\n",
    "    agent_list=agent_list,\n",
    "    llm_config=default_llm_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a30e4b4297edd1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 6 (Optional): clear all agents and prepare for the next task\n",
    "You can clear all agents generated in this task by the following code if your task is completed or the next task is largely different from the current task. If the agent's backbone is an open-source LLM, this process will also shutdown the endpoint server. If necessary, you can use `recycle_endpoint=False` to retain the previous open-source LLMs' endpoint server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb0bfff01dd1330",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T13:34:08.429248500Z",
     "start_time": "2023-12-03T13:34:08.364799400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "builder.clear_all_agents(recycle_endpoint=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb098638a086898",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Save & load configs\n",
    "\n",
    "You can save all necessary information of the built group chat agents. Here is a case for those agents generated in the above task:\n",
    "```json\n",
    "{\n",
    "    \"building_task\": \"Find a paper on arxiv by programming, and analysis its application in some domain. For example, find a latest paper about gpt-4 on arxiv and find its potential applications in software.\",\n",
    "    \"agent_configs\": [\n",
    "        {\n",
    "            \"name\": \"Data_scientist\",\n",
    "            \"model\": \"gpt-4-1106-preview\",\n",
    "            \"system_message\": \"As a Data Scientist, you are tasked with automating the retrieval and analysis of academic papers from arXiv. Utilize your Python programming acumen to develop scripts for gathering necessary information such as searching for relevant papers, downloading them, and processing their contents. Apply your analytical and language skills to interpret the data and deduce the applications of the research within specific domains.\\n\\n1. To compile information, write and implement Python scripts that search and interact with online resources, download and read files, extract content from documents, and perform other information-gathering tasks. Use the printed output as the foundation for your subsequent analysis.\\n\\n2. Execute tasks programmatically with Python scripts when possible, ensuring results are directly displayed. Approach each task with efficiency and strategic thinking.\\n\\nProgress through tasks systematically. In instances where a strategy is not provided, outline your plan before executing. Clearly distinguish between tasks handled via code and those utilizing your analytical expertise.\\n\\nWhen providing code, include only Python scripts meant to be run without user alterations. Users should execute your script as is, without modifications:\\n\\n```python\\n# filename: <filename>\\n# Python script\\nprint(\\\"Your output\\\")\\n```\\n\\nUsers should not perform any actions other than running the scripts you provide. Avoid presenting partial or incomplete scripts that require user adjustments. Refrain from requesting users to copy-paste results; instead, use the 'print' function when suitable to display outputs. Monitor the execution results they share.\\n\\nIf an error surfaces, supply corrected scripts for a re-run. If the strategy fails to resolve the issue, reassess your assumptions, gather additional details as needed, and explore alternative approaches.\\n\\nUpon successful completion of a task and verification of the results, confirm the achievement of the stated objective. Ensuring accuracy and validity of the findings is paramount. Evidence supporting your conclusions should be provided when feasible.\\n\\nUpon satisfying the user's needs and ensuring all tasks are finalized, conclude your assistance with \\\"TERMINATE\\\".\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Research_analyst\",\n",
    "            \"model\": \"gpt-4-1106-preview\",\n",
    "            \"system_message\": \"As a Research Analyst, you are expected to be a proficient AI assistant possessing a strong grasp of programming, specifically in Python, and robust analytical capabilities. Your primary responsibilities will include:\\n\\n1. Conducting comprehensive searches and retrieving information autonomously through Python scripts, such as querying databases, accessing web services (like arXiv), downloading and reading files, and retrieving system information.\\n2. Analyzing the content of the retrieved documents, particularly academic papers, and extracting insights regarding their application in specific domains, such as the potential uses of GPT-4 in software development.\\n3. Presenting your findings in a clear, detailed manner, explaining the implications of the research and its relevance to the assigned task.\\n4. Employing your programming skills to automate tasks where possible, ensuring the output is delivered through Python code with clear, executable instructions. Your code will be designed for the user to execute without amendment or additional input.\\n5. Verifying the results of information gathering and analysis to ensure accuracy and completeness, providing evidence to support your conclusions when available.\\n6. Communicating the completion of each task and confirming that the user's needs have been satisfied through a clear and conclusive statement, followed by the word \\\"TERMINATE\\\" to signal the end of the interaction.\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Software_developer\",\n",
    "            \"model\": \"gpt-4-1106-preview\",\n",
    "            \"system_message\": \"As a dedicated AI assistant for a software developer, your role involves employing your Python programming prowess and proficiency in natural language processing to facilitate the discovery and analysis of scholarly articles on arXiv. Your tasks include crafting Python scripts to automatically search, retrieve, and present information regarding the latest research, with a focus on applicable advancements in technology such as GPT-4 and its potential impact on the domain of software development.\\n\\n1. Utilize Python to programmatically seek out and extract pertinent data, for example, navigating or probing the web, downloading/ingesting documents, or showcasing content from web pages or files. When enough information has been accumulated to proceed, you will then analyze and interpret the findings.\\n\\n2. When there's a need to perform an operation programmatically, your Python code should accomplish the task and manifest the outcome. Progress through the task incrementally and systematically.\\n\\nProvide a clear plan outlining each stage of the task, specifying which components will be executed through Python coding and which through your linguistic capabilities. When proposing Python code, remember to:\\n\\n- Label the script type within the code block\\n- Avoid suggesting code that the user would need to alter\\n- Refrain from including more than one code block in your response\\n- Circumvent requesting the user to manually transcribe any results; utilize 'print' statements where applicable\\n- Examine the user's reported execution outcomes\\n\\nIf an error arises, your responsibility is to rectify the issue and submit the corrected script. Should an error remain unresolvable, or if the task remains incomplete post successful code execution, re-evaluate the scenario, gather any further required information, and formulate an alternative approach.\\n\\nUpon confirming that the task has been satisfactorily accomplished and the user's requirements have been met, indicate closure of the procedure with a concluding statement.\"\n",
    "        }\n",
    "    ],\n",
    "    \"coding\": true,\n",
    "    \"default_llm_config\": {\n",
    "        \"temperature\": 0\n",
    "    }\n",
    "}\n",
    "```\n",
    "These information will be saved in JSON format. You can provide a specific filename, otherwise, AgentBuilder will save config to the current path with a generated filename 'save_config_TASK_MD5.json'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b88a5d482ceba4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T13:34:09.652503400Z",
     "start_time": "2023-12-03T13:34:09.639760500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "saved_path = builder.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35620c10ee42be",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "After that, you can load the saved config and skip the building process. AgentBuilder will create agents with those information without prompting the builder manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34addd498e5ab174",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T13:34:56.857918300Z",
     "start_time": "2023-12-03T13:34:11.108958800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_builder = AgentBuilder(config_path=config_path)\n",
    "agent_list, agent_configs = new_builder.load(saved_path)  # load previous agent configs\n",
    "start_task(\n",
    "    execution_task=\"Find a recent paper about Llava on arxiv and find its potential applications in computer vision.\",\n",
    "    agent_list=agent_list,\n",
    "    llm_config=default_llm_config\n",
    ")\n",
    "new_builder.clear_all_agents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e0cf8f09eef5cd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Use OpenAI Assistant\n",
    "\n",
    "[The Assistants API](https://platform.openai.com/docs/assistants/overview) allows you to build AI assistants within your own applications. An Assistant has instructions and can leverage models, tools, and knowledge to respond to user queries.\n",
    "AutoBuild also support assistant api by adding `use_oai_assistant=True` to `build()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4051c25b2cd1918c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T13:59:35.497212500Z",
     "start_time": "2023-12-03T13:47:45.765859300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_builder = AgentBuilder(config_path=config_path)\n",
    "agent_list, agent_configs = new_builder.build(building_task, default_llm_config, use_oai_assistant=True)  # Transfer to OpenAI assistant API.\n",
    "start_task(\n",
    "    execution_task=\"Find a recent paper about XAI on arxiv and find its potential applications in medical.\",\n",
    "    agent_list=agent_list,\n",
    "    llm_config=default_llm_config\n",
    ")\n",
    "new_builder.clear_all_agents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbfef9268fc5191",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
