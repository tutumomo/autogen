{"type":"assistant","config":{"name":"default assistant","llm_config":{"config_list":[{"user_id":"guestuser@gmail.com","timestamp":"2024-02-11T17:22:28.909082","model":"gpt-3.5-turbo-1106","base_url":null,"api_type":null,"api_version":null,"description":"gpt-3.5-turbo\ngpt-3.5-turbo-1106\ngpt-3.5-turbo-16k\n"}],"temperature":0.1,"cache_seed":null,"timeout":null},"human_input_mode":"NEVER","max_consecutive_auto_reply":8,"system_message":"You are a helpful AI assistant. Solve tasks using your coding and language skills. In the following cases, suggest python code (in a python coding block) or shell script (in a cmd coding block) for the user to execute. 1. When you need to collect info, use the code to output the info you need, for example, browse or search the web, download/read a file, print the content of a webpage or a file, get the current date/time, check the operating system. After sufficient info is printed and the task is ready to be solved based on your language skill, you can solve the task by yourself. 2. When you need to perform some task with code, use the code to perform the task and output the result. Finish the task smartly. Solve the task step by step if you need to. If a plan is not provided, explain your plan first. Be clear which step uses code, and which step uses your language skill. When using code, you must indicate the script type in the code block. The user cannot provide any other feedback or perform any other action beyond executing the code you suggest. The user can't modify your code. So do not suggest incomplete code which requires users to modify. Don't use a code block if it's not intended to be executed by the user. If you want the user to save the code in a file before executing it, put # filename: <filename> inside the code block as the first line. Don't include multiple code blocks in one response. Do not ask users to copy and paste the result. Instead, use 'print' function for the output when relevant. Check the execution result returned by the user. If the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error can't be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try. When you find an answer, verify the answer carefully. Include verifiable evidence in your response if possible. Reply 'TERMINATE' in the end when everything is done. 全部過程包含中間過程對話、程式註解及最後結果均使用繁體中文。","is_termination_msg":null,"code_execution_config":false,"default_auto_reply":"TERMINATE"},"timestamp":"2024-02-11T17:48:29.137875","user_id":"guestuser@gmail.com","skills":[{"title":"find_papers_arxiv","content":"\"\"\"\nfind_papers_arxiv：\n這段代碼的目的是使用arXiv API從arXiv學術文獻數據庫中搜索相關論文，並將搜索結果保存到本地緩存中，以便日後快速訪問。以下是代碼的逐步解釋：\n\n導入所需模塊：\n\nos：用於操作系統相關功能，例如創建目錄。\nre：正則表達式模塊，用於處理字符串。\njson：處理JSON格式數據。\nhashlib：提供數據加密功能，這裡用於生成唯一的緩存鍵。\narxiv：arXiv API 的一個接口，用於搜索arXiv數據庫。\n定義 search_arxiv 函數：\n\nquery：搜索查詢字符串。\nmax_results：最多返回的結果數量，預設為10。\n生成緩存鍵：使用MD5對搜索查詢進行加密，以創建獨特的緩存文件名。\n\n創建緩存目錄：如果 .cache 目錄不存在，則創建它。\n\n檢查緩存：如果對應緩存文件存在，則從緩存讀取數據並返回。\n\n處理搜索查詢：\n\n使用正則表達式移除查詢中的特殊字符和操作符（如“and”, “or”, “not”）。\n將查詢轉換為小寫並去除多餘空格。\n執行搜索：使用arXiv模塊執行實際的搜索。\n\n處理搜索結果：\n\n將每篇論文的資訊（如標題、作者、摘要等）整理為字典格式。\n將這些字典添加到結果列表中。\n限制結果數量：如果結果數量超過 max_results，則截斷列表。\n\n保存結果到緩存：將搜索結果以JSON格式保存到緩存文件中。\n\n返回結果：返回處理後的搜索結果列表。\n\n整體而言，這個函數提供了從arXiv檢索學術文獻並將結果緩存到本地的功能，這可以提高重複查詢的效率。\n\"\"\"\n\nimport os\nimport re\nimport json\nimport hashlib\n\ndef search_arxiv(query, max_results=10):\n    \"\"\"\n    Searches arXiv for the given query using the arXiv API, then returns the search results. This is a helper function. In most cases, callers will want to use 'find_relevant_papers( query, max_results )' instead.\n\n    Args:\n        query (str): The search query.\n        max_results (int, optional): The maximum number of search results to return. Defaults to 10.\n\n    Returns:\n        jresults (list): A list of dictionaries. Each dictionary contains fields such as 'title', 'authors', 'summary', and 'pdf_url'\n\n    Example:\n        >>> results = search_arxiv(\"attention is all you need\")\n        >>> print(results)\n    \"\"\"\n\n    import arxiv\n\n    key = hashlib.md5((\"search_arxiv(\" + str(max_results) + \")\" + query).encode(\"utf-8\")).hexdigest()\n    # Create the cache if it doesn't exist\n    cache_dir = \".cache\"\n    if not os.path.isdir(cache_dir):\n        os.mkdir(cache_dir)\n\n    fname = os.path.join(cache_dir, key + \".cache\")\n\n    # Cache hit\n    if os.path.isfile(fname):\n        fh = open(fname, \"r\", encoding=\"utf-8\")\n        data = json.loads(fh.read())\n        fh.close()\n        return data\n\n    # Normalize the query, removing operator keywords\n    query = re.sub(r\"[^\\s\\w]\", \" \", query.lower())\n    query = re.sub(r\"\\s(and|or|not)\\s\", \" \", \" \" + query + \" \")\n    query = re.sub(r\"[^\\s\\w]\", \" \", query.lower())\n    query = re.sub(r\"\\s+\", \" \", query).strip()\n\n    search = arxiv.Search(query=query, max_results=max_results, sort_by=arxiv.SortCriterion.Relevance)\n\n    jresults = list()\n    for result in search.results():\n        r = dict()\n        r[\"entry_id\"] = result.entry_id\n        r[\"updated\"] = str(result.updated)\n        r[\"published\"] = str(result.published)\n        r[\"title\"] = result.title\n        r[\"authors\"] = [str(a) for a in result.authors]\n        r[\"summary\"] = result.summary\n        r[\"comment\"] = result.comment\n        r[\"journal_ref\"] = result.journal_ref\n        r[\"doi\"] = result.doi\n        r[\"primary_category\"] = result.primary_category\n        r[\"categories\"] = result.categories\n        r[\"links\"] = [str(link) for link in result.links]\n        r[\"pdf_url\"] = result.pdf_url\n        jresults.append(r)\n\n    if len(jresults) > max_results:\n        jresults = jresults[0:max_results]\n\n    # Save to cache\n    fh = open(fname, \"w\")\n    fh.write(json.dumps(jresults))\n    fh.close()\n    return jresults\n","file_name":null,"description":null,"timestamp":"2024-02-11T17:31:38.880876","user_id":"default"},{"title":"SearchTools","content":"\"\"\"\nSearchTools：\n這段代碼定義了一個名為 SearchTools 的 Python 類別，其中包含兩個用於進行網路搜尋的方法：search_internet 和 search_news。這些方法通過一個外部 API 實現對互聯網和新聞的搜索。以下是對代碼的逐行解釋：\n\n共通部分：\n\nimport json - 引入 Python 的 json 模組，用於處理 JSON 格式的數據。\nimport os - 引入 os 模組，用於訪問操作系統的功能，例如環境變量。\nimport requests - 引入 requests 模組，用於發送 HTTP 請求。\nfrom langchain.tools import tool - 從 langchain.tools 模組中引入 tool 裝飾器，用於定義工具方法。\nsearch_internet 方法：\n\n使用 @tool(\"Search the internet\") 裝飾器。\n方法接受一個參數 query，表示要搜索的查詢內容。\n定義要返回的最高結果數量。\n設定 API URL 為 Google SERPer（一種搜索引擎結果頁面解析服務）的搜索接口。\n創建一個 payload 包含搜索查詢，並轉換為 JSON 格式。\n設定 HTTP 請求頭，包括 API 密鑰和內容類型。\n使用 requests 發送 POST 請求並接收響應。\n從響應中提取有關搜索結果的數據。\n遍歷並處理最多四個搜索結果，格式化為包含標題、鏈接和摘要的字符串。\n返回格式化的搜索結果。\nsearch_news 方法：\n\n與 search_internet 方法類似，但專注於搜索新聞。\n方法使用不同的 API URL (https://google.serper.dev/news) 進行新聞搜索。\n其他操作與\nsearch_internet 方法類似，包括創建 payload、發送請求、處理響應、格式化結果。\n\n總結：\n\n這兩個方法都使用了第三方 API（SERPer API），透過 Google 搜索引擎進行網絡和新聞的搜索。\n方法中的錯誤處理（例如 try-except 塊和 next）用於處理潛在的鍵值缺失情況。\n返回的結果包括搜索到的項目的標題、鏈接和摘要，格式化為易於閱讀的字符串。\n這些工具類別的方法可以在需要自動化搜索互聯網或新聞資訊的應用場景中使用。\n整體而言，這段代碼展示了如何利用外部 API 和 Python 編程來實現自動化的網絡搜尋功能，對於需要快速獲取和整理網上資訊的場景特別有用。\n\"\"\"\nimport json\nimport os\n\nimport requests\nfrom langchain.tools import tool\n\nclass SearchTools():\n  @tool(\"Search the internet\")\n  def search_internet(query):\n    \"\"\"Useful to search the internet \n    about a a given topic and return relevant results\"\"\"\n    top_result_to_return = 4\n    url = \"https://google.serper.dev/search\"\n    payload = json.dumps({\"q\": query})\n    headers = {\n        'X-API-KEY': os.environ['SERPER_API_KEY'],\n        'content-type': 'application/json'\n    }\n    response = requests.request(\"POST\", url, headers=headers, data=payload)\n    results = response.json()['organic']\n    string = []\n    for result in results[:top_result_to_return]:\n      try:\n        string.append('\\n'.join([\n            f\"Title: {result['title']}\", f\"Link: {result['link']}\",\n            f\"Snippet: {result['snippet']}\", \"\\n-----------------\"\n        ]))\n      except KeyError:\n        next\n\n    return '\\n'.join(string)\n\n  @tool(\"Search news on the internet\")\n  def search_news(query):\n    \"\"\"Useful to search news about a company, stock or any other\n    topic and return relevant results\"\"\"\"\"\n    top_result_to_return = 4\n    url = \"https://google.serper.dev/news\"\n    payload = json.dumps({\"q\": query})\n    headers = {\n        'X-API-KEY': os.environ['SERPER_API_KEY'],\n        'content-type': 'application/json'\n    }\n    response = requests.request(\"POST\", url, headers=headers, data=payload)\n    results = response.json()['news']\n    string = []\n    for result in results[:top_result_to_return]:\n      try:\n        string.append('\\n'.join([\n            f\"Title: {result['title']}\", f\"Link: {result['link']}\",\n            f\"Snippet: {result['snippet']}\", \"\\n-----------------\"\n        ]))\n      except KeyError:\n        next\n\n    return '\\n'.join(string)","file_name":null,"description":null,"timestamp":"2024-02-11T17:31:17.015992","user_id":"default"},{"title":"SECTools","content":"\"\"\"\nSECTools：\n這段代碼定義了一個名為 SECTools 的 Python 類別，該類別包含了兩個方法用於搜尋和分析美國證券交易委員會（SEC）提交的報告，特別是 10-Q 和 10-K 表格。以下是對代碼的逐行解釋：\n\n通用部分\nimport os - 引入 os 模組，用於訪問操作系統的功能，例如環境變量。\nimport requests - 引入 requests 模組，用於發送 HTTP 請求。\nfrom langchain.tools import tool - 從 langchain.tools 模組中引入 tool 裝飾器，用於定義工具方法。\nfrom langchain.text_splitter import CharacterTextSplitter - 引入用於文本分割的工具。\nfrom langchain.embeddings import OpenAIEmbeddings - 引入用於生成文本嵌入的 OpenAI 嵌入模型。\nfrom langchain_community.vectorstores import FAISS - 引\n入用於建立向量存儲的 FAISS 庫，以便進行高效的相似性搜索。\n7. from sec_api import QueryApi - 引入用於查詢 SEC 數據的 API 模塊。\n\nfrom unstructured.partition.html import partition_html - 引入用於處理 HTML 文檔的函數。\nsearch_10q 方法\n使用 @tool(\"Search 10-Q form\") 襝飾器。\n接收一個由股票代碼和查詢問題組成的字符串，用管道符（|）分隔。\n使用 SEC API（通過環境變量中的 API 密鑰）構造查詢，以獲取指定股票的最新 10-Q 表格。\n如果找不到相應的文件，返回一個錯誤信息。\n否則，從返回的文件中提取鏈接，並使用 __embedding_search 方法來搜索和提取相關的回答。\nsearch_10k 方法\n類似於 search_10q，但用於搜索 10-K 表格（年度報告）。\n__embedding_search 私有方法\n下載並處理指定 URL 的 HTML 內容。\n使用 partition_html 函數將 HTML 內容分割成多個元素。\n使用 CharacterTextSplitter 將文本劃分為更小的部分，以便處理。\n使用 OpenAIEmbeddings 生成文本的嵌入表示。\n使用 FAISS 庫建立向量存儲，並通過嵌入相似性搜索相關答案。\n返回前四個最相關的文檔內容作為答案。\n__download_form_html 私有方法\n用於從給定的 URL 下載 HTML 內容。\n設置 HTTP 請求頭以模擬常見的瀏覽器行為，以確保成功獲取頁面內容。\n總結：\n這個類通過 SEC API 搜索特定股票的 10-Q 和 10-K 報告，解析報告內容，並使用文本嵌入和向量相似性搜索來回答關於報告內容的特定問題。\n這是一個自動化工具，適用於金融分析師或投資者，用於快速獲取和理解公司的財務報告。\n\"\"\"\nimport os\nimport requests\nfrom langchain.tools import tool\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\n\nfrom sec_api import QueryApi\nfrom unstructured.partition.html import partition_html\n\nclass SECTools():\n  @tool(\"Search 10-Q form\")\n  def search_10q(data):\n    \"\"\"\n    Useful to search information from the latest 10-Q form for a\n    given stock.\n    The input to this tool should be a pipe (|) separated text of\n    length two, representing the stock ticker you are interested and what\n    question you have from it.\n\t\tFor example, `AAPL|what was last quarter's revenue`.\n    \"\"\"\n    stock, ask = data.split(\"|\")\n    queryApi = QueryApi(api_key=os.environ['SEC_API_API_KEY'])\n    query = {\n      \"query\": {\n        \"query_string\": {\n          \"query\": f\"ticker:{stock} AND formType:\\\"10-Q\\\"\"\n        }\n      },\n      \"from\": \"0\",\n      \"size\": \"1\",\n      \"sort\": [{ \"filedAt\": { \"order\": \"desc\" }}]\n    }\n\n    fillings = queryApi.get_filings(query)['filings']\n    if len(fillings) == 0:\n      return \"Sorry, I couldn't find any filling for this stock, check if the ticker is correct.\"\n    link = fillings[0]['linkToFilingDetails']\n    answer = SECTools.__embedding_search(link, ask)\n    return answer\n\n  @tool(\"Search 10-K form\")\n  def search_10k(data):\n    \"\"\"\n    Useful to search information from the latest 10-K form for a\n    given stock.\n    The input to this tool should be a pipe (|) separated text of\n    length two, representing the stock ticker you are interested, what\n    question you have from it.\n    For example, `AAPL|what was last year's revenue`.\n    \"\"\"\n    stock, ask = data.split(\"|\")\n    queryApi = QueryApi(api_key=os.environ['SEC_API_API_KEY'])\n    query = {\n      \"query\": {\n        \"query_string\": {\n          \"query\": f\"ticker:{stock} AND formType:\\\"10-K\\\"\"\n        }\n      },\n      \"from\": \"0\",\n      \"size\": \"1\",\n      \"sort\": [{ \"filedAt\": { \"order\": \"desc\" }}]\n    }\n\n    fillings = queryApi.get_filings(query)['filings']\n    if len(fillings) == 0:\n      return \"Sorry, I couldn't find any filling for this stock, check if the ticker is correct.\"\n    link = fillings[0]['linkToFilingDetails']\n    answer = SECTools.__embedding_search(link, ask)\n    return answer\n\n  def __embedding_search(url, ask):\n    text = SECTools.__download_form_html(url)\n    elements = partition_html(text=text)\n    content = \"\\n\".join([str(el) for el in elements])\n    text_splitter = CharacterTextSplitter(\n        separator = \"\\n\",\n        chunk_size = 1000,\n        chunk_overlap  = 150,\n        length_function = len,\n        is_separator_regex = False,\n    )\n    docs = text_splitter.create_documents([content])\n    retriever = FAISS.from_documents(\n      docs, OpenAIEmbeddings()\n    ).as_retriever()\n    answers = retriever.get_relevant_documents(ask, top_k=4)\n    answers = \"\\n\\n\".join([a.page_content for a in answers])\n    return answers\n\n  def __download_form_html(url):\n    headers = {\n      'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n      'Accept-Encoding': 'gzip, deflate, br',\n      'Accept-Language': 'en-US,en;q=0.9,pt-BR;q=0.8,pt;q=0.7',\n      'Cache-Control': 'max-age=0',\n      'Dnt': '1',\n      'Sec-Ch-Ua': '\"Not_A Brand\";v=\"8\", \"Chromium\";v=\"120\"',\n      'Sec-Ch-Ua-Mobile': '?0',\n      'Sec-Ch-Ua-Platform': '\"macOS\"',\n      'Sec-Fetch-Dest': 'document',\n      'Sec-Fetch-Mode': 'navigate',\n      'Sec-Fetch-Site': 'none',\n      'Sec-Fetch-User': '?1',\n      'Upgrade-Insecure-Requests': '1',\n      'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n    }\n\n    response = requests.get(url, headers=headers)\n    return response.text","file_name":null,"description":null,"timestamp":"2024-02-11T17:30:58.669470","user_id":"default"},{"title":"BrowserTools","content":"\"\"\"\nBrowserTools：\n這個 tools 不單單是定義了 tools，是在 tools 裡面就定義了 agent、task，並執行了 task.excute()。\n這段代碼定義了一個名為 BrowserTools 的 Python 類別，其中包含一個方法 scrape_and_summarize_website，用於從網站抓取內容並進行摘要。以下是對代碼的逐行解釋：\nimport json - 引入 Python 的 json 模組，用於處理 JSON 格式的數據。\nimport os - 引入 os 模組，用於訪問操作系統功能，比如環境變量。\nimport requests - 引入 requests 模組，用於發送 HTTP 請求。\nfrom crewai import Agent, Task - 從 crewai 模組中引入 Agent 和 Task 類別，這些可能是用於處理自動化任務或人工智能相關功能的類別。\nfrom langchain.tools import tool - 從 langchain.tools 模組中引入 tool 裝飾器，用於定義工具方法。\nfrom unstructured.partition.html import partition_html - 引入用於處理 HTML 內容的 partition_html 函數。\nBrowserTools 類別定義了 scrape_and_summarize_website 方法：\n使用 @tool(\"Scrape website content\") 裝飾器。\n方法接受一個參數 website，表示要抓取內容的網站 URL。\n構建一個用於向 browserless API 發送請求的 URL，其中包含從環境變量獲取的 API 密鑰。\n創建一個 payload，其中包含要抓取的網站 URL，並將其轉換為 JSON 格式。\n設定 HTTP 請求頭。\n使用 requests 發送 POST 請求到 browserless API，獲取網站的 HTML 內容。\n使用 partition_html 函數處理獲取的 HTML 文本，將其分割為多個元素。\n進行文本處理，將元素轉換為字符串並分割成大小為 8000 字符的塊。\n對每個文本塊進行摘要處理：\n創建一個 Agent 並設置其角色、目標和背景故事。\n創建一個 Task，\n其中包括 Agent 和對文本塊進行摘要的描述。\n執行 Task 來產生摘要。\n將所有摘要收集到一個列表中。\n最後，將所有摘要連接成一個字符串並返回。\n這個方法的主要功能是使用 browserless API 從給定的網站抓取 HTML 內容，然後將這些內容分割、處理並通過 AI 代理（如 Agent 和 Task）生成摘要。這個過程可能用於自動化地從網站收集並簡化信息，特別是在需要快速獲得網站主要內容摘要的情況下。整個過程都以程式化的方式進行，顯示了現代技術在資訊處理和自動化方面的應用。\n\"\"\"\nimport json\nimport os\n\nimport requests\nfrom crewai import Agent, Task\nfrom langchain.tools import tool\nfrom unstructured.partition.html import partition_html\n\nclass BrowserTools():\n\n  @tool(\"Scrape website content\")\n  def scrape_and_summarize_website(website):\n    \"\"\"Useful to scrape and summarize a website content\"\"\"\n    url = f\"https://chrome.browserless.io/content?token={os.environ['BROWSERLESS_API_KEY']}\"\n    payload = json.dumps({\"url\": website})\n    headers = {'cache-control': 'no-cache', 'content-type': 'application/json'}\n    response = requests.request(\"POST\", url, headers=headers, data=payload)\n    elements = partition_html(text=response.text)\n    content = \"\\n\\n\".join([str(el) for el in elements])\n    content = [content[i:i + 8000] for i in range(0, len(content), 8000)]\n    summaries = []\n    for chunk in content:\n      agent = Agent(\n          role='Principal Researcher',\n          goal=\n          'Do amazing research and summaries based on the content you are working with',\n          backstory=\n          \"You're a Principal Researcher at a big company and you need to do research about a given topic.\",\n          allow_delegation=False)\n      task = Task(\n          agent=agent,\n          description=\n          f'Analyze and summarize the content below, make sure to include the most relevant information in the summary, return only the summary nothing else.\\n\\nCONTENT\\n----------\\n{chunk}'\n      )\n      summary = task.execute()\n      summaries.append(summary)\n    return \"\\n\\n\".join(summaries)","file_name":null,"description":null,"timestamp":"2024-02-11T17:30:29.151816","user_id":"default"},{"title":"CalculatorTools","content":"\"\"\"\nCalculatorTools：\n這段代碼看起來是Python程式碼，它似乎定義了一個名為CalculatorTools的類別，該類別具有一個裝飾器（@tool(\"Make a calculation\")）和一個名為calculate的方法。\n讓我們一步步解釋這段代碼：\nfrom langchain.tools import tool: 這個語句導入了一個叫做tool的模組或函式，它似乎是用來定義一些工具或功能的。\nclass CalculatorTools(): 這行代碼定義了一個名為CalculatorTools的Python類別。\n@tool(\"Make a calculation\"): 這是一個裝飾器，它將下一個方法calculate標記為一個工具，並提供了一個描述，即\"Make a calculation\"。這個裝飾器的作用可能是將calculate方法註冊為一個可用的工具，以便在其他地方使用。\ndef calculate(operation): 這是一個方法定義，它接受一個名為operation的參數。根據註釋，這個方法用於執行數學計算，例如加法、減法、乘法、除法等。它使用Python的eval函式來評估傳遞給它的operation，這個operation應該是一個數學表達式，例如200*7或5000/2*10。評估結果將被返回。\n總結來說，這個程式碼看起來是定義了一個名為CalculatorTools的類別，該類別具有一個用於執行數學計算的工具方法calculate，這個工具方法可以接受一個數學表達式並返回計算結果。該代碼還使用了tool裝飾器，可能是將calculate方法註冊為一個可用的工具，以便在其他地方使用。\n\"\"\"\nfrom langchain.tools import tool\n\nclass CalculatorTools():\n\n  @tool(\"Make a calcualtion\")\n  def calculate(operation):\n    \"\"\"Useful to perform any mathematical calculations, \n    like sum, minus, multiplication, division, etc.\n    The input to this tool should be a mathematical \n    expression, a couple examples are `200*7` or `5000/2*10`\n    \"\"\"\n    return eval(operation)","file_name":null,"description":null,"timestamp":"2024-02-11T17:30:13.471733","user_id":"default"},{"title":"FileTools","content":"\"\"\"\nFileTools：\n這段代碼看起來是Python程式碼，它定義了一個名為FileTools的類別，並在該類別中定義了一個方法write_file。以下是這段代碼的解釋：\nfrom langchain.tools import tool: 這個語句導入了一個叫做tool的模組或函式，它似乎是用來定義一些工具或功能的。\nclass FileTools(): 這行代碼定義了一個名為FileTools的Python類別。\n@tool(\"Write File with content\"): 這是一個裝飾器，它將下一個方法write_file標記為一個工具，並提供了一個描述，即\"Write File with content\"。這個裝飾器的作用可能是將write_file方法註冊為一個可用的工具，以便在其他地方使用。\ndef write_file(data): 這是一個方法定義，它接受一個名為data的參數。根據註釋，這個方法用於將特定內容寫入文件。它接受一個data參數，這個data應該是一個使用管道符號（|）分隔的文本，該文本包含兩部分：文件的完整路徑（包括/workdir/template）和要寫入文件的React組件代碼內容。例如，./Keynote/src/components/Hero.jsx|REACT_COMPONENT_CODE_PLACEHOLDER。你需要將REACT_COMPONENT_CODE_PLACEHOLDER替換為你想要寫入文件的實際代碼。\n在try塊中，代碼首先試圖使用|分隔data，以獲取文件路徑和React組件代碼。\n接下來，代碼對文件路徑進行一些處理，例如刪除換行符號、空格和反引號。然後，如果文件路徑不以\"./workdir\"開頭，它會將路徑修改為以\"./workdir\"開頭，這可能是相對於某個工作目錄的路徑。\n最後，代碼使用open函式打開文件，將React組件代碼寫入文件，然後返回一條成功的消息，指示文件已經寫入。\n如果在任何步驟中出現異常，則except塊中的代碼會捕獲異常並返回一條錯誤消息，指示輸入格式有問題。\n總結來說，這段代碼定義了一個名為FileTools的類別，該類別包含一個方法write_file，用於將React組件代碼寫入指定的文件中。該方法接受一個特定格式的文本作為參數，並在處理過程中處理文件路徑，然後將代碼寫入文件中。如果一切順利，它將返回成功的消息，否則將返回錯誤消息。\n\"\"\"\nfrom langchain.tools import tool\n\nclass FileTools():\n\n  @tool(\"Write File with content\")\n  def write_file(data):\n    \"\"\"Useful to write a file to a given path with a given content. \n       The input to this tool should be a pipe (|) separated text \n       of length two, representing the full path of the file, \n       including the /workdir/template, and the React \n       Component code content you want to write to it.\n       For example, `./Keynote/src/components/Hero.jsx|REACT_COMPONENT_CODE_PLACEHOLDER`.\n       Replace REACT_COMPONENT_CODE_PLACEHOLDER with the actual \n       code you want to write to the file.\"\"\"\n    try:\n      path, content = data.split(\"|\")\n      path = path.replace(\"\\n\", \"\").replace(\" \", \"\").replace(\"`\", \"\")\n      if not path.startswith(\"./workdir\"):\n        path = f\"./workdir/{path}\"\n      with open(path, \"w\") as f:\n        f.write(content)\n      return f\"File written to {path}.\"\n    except Exception:\n      return \"Error with the input format for the tool.\"","file_name":null,"description":null,"timestamp":"2024-02-11T17:29:58.713721","user_id":"default"},{"title":"ExaSearchTool","content":"\"\"\"\nExaSearchTool 這段程式碼是一個使用了名為Exa的Python套件，並利用langchain套件中的工具，來進行網頁搜尋、查找相似網頁以及獲取網頁內容的類別定義。\n讓我們逐行解釋：\nimport os: 導入了Python的os模組，用於與作業系統進行互動。\nfrom exa_py import Exa: 從exa_py模組中導入了Exa類別，用於執行網頁相關操作。\nfrom langchain.agents import tool: 從langchain.agents模組中導入了tool裝飾器，用於定義工具函數。\nclass ExaSearchTool:: 定義了一個名為ExaSearchTool的類別，用於包裝與Exa和langchain相關操作的方法。\n@tool: 這是一個裝飾器，標記了以下定義的方法是一個工具函數，可以被langchain套件使用。\ndef search(query: str):: 定義了一個名為search的方法，該方法接受一個字串型態的查詢參數，並返回相關的網頁搜索結果。\ndef find_similar(url: str):: 定義了一個名為find_similar的方法，該方法接受一個字串型態的URL參數，並返回與該URL相似的其他網頁結果。\ndef get_contents(ids: str):: 定義了一個名為get_contents的方法，該方法接受一個字串型態的ID參數，並返回該ID對應的網頁內容。這裡使用了eval函數來將字串型態的IDs轉換為列表型態。然後該方法會輸出網頁的內容並返回前1000個字元，並將多個內容以換行符分隔並返回。\n\"\"\"\nimport os\nfrom exa_py import Exa\nfrom langchain.agents import tool\n\nclass ExaSearchTool:\n\t@tool\n\tdef search(query: str):\n\t\t\"\"\"Search for a webpage based on the query.\"\"\"\n\t\treturn ExaSearchTool._exa().search(f\"{query}\", use_autoprompt=True, num_results=3)\n\n\t@tool\n\tdef find_similar(url: str):\n\t\t\"\"\"Search for webpages similar to a given URL.\n\t\tThe url passed in should be a URL returned from `search`.\n\t\t\"\"\"\n\t\treturn ExaSearchTool._exa().find_similar(url, num_results=3)\n\n\t@tool\n\tdef get_contents(ids: str):\n\t\t\"\"\"Get the contents of a webpage.\n\t\tThe ids must be passed in as a list, a list of ids returned from `search`.\n\t\t\"\"\"\n\t\tids = eval(ids)\n\t\tcontents = str(ExaSearchTool._exa().get_contents(ids))\n\t\tprint(contents)\n\t\tcontents = contents.split(\"URL:\")\n\t\tcontents = [content[:1000] for content in contents]\n\t\treturn \"\\n\\n\".join(contents)\n\n\tdef tools():\n\t\treturn [ExaSearchTool.search, ExaSearchTool.find_similar, ExaSearchTool.get_contents]\n\n\tdef _exa():\n\t\treturn Exa(api_key=os.environ[\"EXA_API_KEY\"])","file_name":null,"description":null,"timestamp":"2024-02-11T17:29:41.961468","user_id":"default"},{"title":"yahoo_finance_news_tool","content":"# YahooFinanceNewsTool 的 tools\n# !pip install langchain_community\nfrom langchain_community.tools.yahoo_finance_news import YahooFinanceNewsTool\nyahoo_finance_news_tool = YahooFinanceNewsTool()","file_name":null,"description":null,"timestamp":"2024-02-11T17:29:14.592873","user_id":"default"},{"title":"search_tool","content":"# DuckDuckGoSearchRun 的 tools\n# !pip install -U duckduckgo-search\nfrom langchain.tools import DuckDuckGoSearchRun\nsearch_tool = DuckDuckGoSearchRun()","file_name":null,"description":null,"timestamp":"2024-02-11T17:29:00.582016","user_id":"default"},{"title":"save_cat_ascii_art_to_png","content":"\n  ## This is a sample skill. Replace with your own skill function\n  ## In general, a good skill must have 3 sections:\n  ## 1. Imports (import libraries needed for your skill)\n  ## 2. Function definition  AND docstrings (this helps the LLM understand what the function does and how to use it)\n  ## 3. Function body (the actual code that implements the function)\n\n  import numpy as np\n  import matplotlib.pyplot as plt\n  from matplotlib import font_manager as fm\n\n  def save_cat_ascii_art_to_png(filename='ascii_cat.png'):\n      \"\"\"\n      Creates ASCII art of a cat and saves it to a PNG file.\n\n      :param filename: str, the name of the PNG file to save the ASCII art.\n      \"\"\"\n      # ASCII art string\n      cat_art = [\n          \"  /_/  \",\n          \" ( o.o ) \",\n          \" > ^ <  \"\n      ]\n\n      # Determine shape of output array\n      height = len(cat_art)\n      width = max(len(line) for line in cat_art)\n\n      # Create a figure and axis to display ASCII art\n      fig, ax = plt.subplots(figsize=(width, height))\n      ax.axis('off')  # Hide axes\n\n      # Get a monospace font\n      prop = fm.FontProperties(family='monospace')\n\n      # Display ASCII art using text\n      for y, line in enumerate(cat_art):\n          ax.text(0, height-y-1, line, fontproperties=prop, fontsize=12)\n\n      # Adjust layout\n      plt.tight_layout()\n\n      # Save figure to file\n      plt.savefig(filename, dpi=120, bbox_inches='tight', pad_inches=0.1)\n      plt.close(fig)","file_name":null,"description":null,"timestamp":"2024-02-11T17:28:39.606514","user_id":"default"},{"title":"generate_images","content":"from typing import List\nimport uuid\nimport requests  # to perform HTTP requests\nfrom pathlib import Path\n\nfrom openai import OpenAI\n\n\ndef generate_and_save_images(query: str, image_size: str = \"1024x1024\") -> List[str]:\n    \"\"\"\n    Function to paint, draw or illustrate images based on the users query or request. Generates images from a given query using OpenAI's DALL-E model and saves them to disk.  Use the code below anytime there is a request to create an image.\n\n    :param query: A natural language description of the image to be generated.\n    :param image_size: The size of the image to be generated. (default is \"1024x1024\")\n    :return: A list of filenames for the saved images.\n    \"\"\"\n\n    client = OpenAI()  # Initialize the OpenAI client\n    response = client.images.generate(model=\"dall-e-3\", prompt=query, n=1, size=image_size)  # Generate images\n\n    # List to store the file names of saved images\n    saved_files = []\n\n    # Check if the response is successful\n    if response.data:\n        for image_data in response.data:\n            # Generate a random UUID as the file name\n            file_name = str(uuid.uuid4()) + \".png\"  # Assuming the image is a PNG\n            file_path = Path(file_name)\n\n            img_url = image_data.url\n            img_response = requests.get(img_url)\n            if img_response.status_code == 200:\n                # Write the binary content to a file\n                with open(file_path, \"wb\") as img_file:\n                    img_file.write(img_response.content)\n                    print(f\"Image saved to {file_path}\")\n                    saved_files.append(str(file_path))\n            else:\n                print(f\"Failed to download the image from {img_url}\")\n    else:\n        print(\"No image data found in the response!\")\n\n    # Return the list of saved files\n    return saved_files\n\n\n# Example usage of the function:\n# generate_and_save_images(\"A cute baby sea otter\")\n","file_name":null,"description":null,"timestamp":"2024-02-11T17:20:30.762132","user_id":"default"},{"title":"fetch_profile","content":"from typing import Optional\nimport requests\nfrom bs4 import BeautifulSoup\n\n\ndef fetch_user_profile(url: str) -> Optional[str]:\n    \"\"\"\n    Fetches the text content from a personal website.\n\n    Given a URL of a person's personal website, this function scrapes\n    the content of the page and returns the text found within the <body>.\n\n    Args:\n        url (str): The URL of the person's personal website.\n\n    Returns:\n        Optional[str]: The text content of the website's body, or None if any error occurs.\n    \"\"\"\n    try:\n        # Send a GET request to the URL\n        response = requests.get(url)\n        # Check for successful access to the webpage\n        if response.status_code == 200:\n            # Parse the HTML content of the page using BeautifulSoup\n            soup = BeautifulSoup(response.text, \"html.parser\")\n            # Extract the content of the <body> tag\n            body_content = soup.find(\"body\")\n            # Return all the text in the body tag, stripping leading/trailing whitespaces\n            return \" \".join(body_content.stripped_strings) if body_content else None\n        else:\n            # Return None if the status code isn't 200 (success)\n            return None\n    except requests.RequestException:\n        # Return None if any request-related exception is caught\n        return None\n","file_name":null,"description":null,"timestamp":"2024-02-11T17:20:30.761130","user_id":"default"}],"description":"default assistant"}