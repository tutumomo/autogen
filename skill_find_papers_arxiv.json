{"title":"find_papers_arxiv",
"content":"\"\"\"\nautogenstudio 預設 skills 之一。\nfind_papers_arxiv：\n這段代碼的目的是使用arXiv API從arXiv學術文獻數據庫中搜索相關論文，並將搜索結果保存到本地緩存中，以便日後快速訪問。以下是代碼的逐步解釋：\n\n導入所需模塊：\n\nos：用於操作系統相關功能，例如創建目錄。\nre：正則表達式模塊，用於處理字符串。\njson：處理JSON格式數據。\nhashlib：提供數據加密功能，這裡用於生成唯一的緩存鍵。\narxiv：arXiv API 的一個接口，用於搜索arXiv數據庫。\n定義 search_arxiv 函數：\n\nquery：搜索查詢字符串。\nmax_results：最多返回的結果數量，預設為10。\n生成緩存鍵：使用MD5對搜索查詢進行加密，以創建獨特的緩存文件名。\n\n創建緩存目錄：如果 .cache 目錄不存在，則創建它。\n\n檢查緩存：如果對應緩存文件存在，則從緩存讀取數據並返回。\n\n處理搜索查詢：\n\n使用正則表達式移除查詢中的特殊字符和操作符（如“and”, “or”, “not”）。\n將查詢轉換為小寫並去除多餘空格。\n執行搜索：使用arXiv模塊執行實際的搜索。\n\n處理搜索結果：\n\n將每篇論文的資訊（如標題、作者、摘要等）整理為字典格式。\n將這些字典添加到結果列表中。\n限制結果數量：如果結果數量超過 max_results，則截斷列表。\n\n保存結果到緩存：將搜索結果以JSON格式保存到緩存文件中。\n\n返回結果：返回處理後的搜索結果列表。\n\n整體而言，這個函數提供了從arXiv檢索學術文獻並將結果緩存到本地的功能，這可以提高重複查詢的效率。\n\"\"\"\nimport os\nimport re\nimport json\nimport hashlib\n\ndef search_arxiv(query, max_results=10):\n    \"\"\"\n    Searches arXiv for the given query using the arXiv API, then returns the search results. This is a helper function. In most cases, callers will want to use 'find_relevant_papers( query, max_results )' instead.\n\n    Args:\n        query (str): The search query.\n        max_results (int, optional): The maximum number of search results to return. Defaults to 10.\n\n    Returns:\n        jresults (list): A list of dictionaries. Each dictionary contains fields such as 'title', 'authors', 'summary', and 'pdf_url'\n\n    Example:\n        >>> results = search_arxiv(\"attention is all you need\")\n        >>> print(results)\n    \"\"\"\n\n    import arxiv\n\n    key = hashlib.md5((\"search_arxiv(\" + str(max_results) + \")\" + query).encode(\"utf-8\")).hexdigest()\n    # Create the cache if it doesn't exist\n    cache_dir = \".cache\"\n    if not os.path.isdir(cache_dir):\n        os.mkdir(cache_dir)\n\n    fname = os.path.join(cache_dir, key + \".cache\")\n\n    # Cache hit\n    if os.path.isfile(fname):\n        fh = open(fname, \"r\", encoding=\"utf-8\")\n        data = json.loads(fh.read())\n        fh.close()\n        return data\n\n    # Normalize the query, removing operator keywords\n    query = re.sub(r\"[^\\s\\w]\", \" \", query.lower())\n    query = re.sub(r\"\\s(and|or|not)\\s\", \" \", \" \" + query + \" \")\n    query = re.sub(r\"[^\\s\\w]\", \" \", query.lower())\n    query = re.sub(r\"\\s+\", \" \", query).strip()\n\n    search = arxiv.Search(query=query, max_results=max_results, sort_by=arxiv.SortCriterion.Relevance)\n\n    jresults = list()\n    for result in search.results():\n        r = dict()\n        r[\"entry_id\"] = result.entry_id\n        r[\"updated\"] = str(result.updated)\n        r[\"published\"] = str(result.published)\n        r[\"title\"] = result.title\n        r[\"authors\"] = [str(a) for a in result.authors]\n        r[\"summary\"] = result.summary\n        r[\"comment\"] = result.comment\n        r[\"journal_ref\"] = result.journal_ref\n        r[\"doi\"] = result.doi\n        r[\"primary_category\"] = result.primary_category\n        r[\"categories\"] = result.categories\n        r[\"links\"] = [str(link) for link in result.links]\n        r[\"pdf_url\"] = result.pdf_url\n        jresults.append(r)\n\n    if len(jresults) > max_results:\n        jresults = jresults[0:max_results]\n\n    # Save to cache\n    fh = open(fname, \"w\")\n    fh.write(json.dumps(jresults))\n    fh.close()\n    return jresults\n",
"file_name":null,
"description":null,
"timestamp":"2024-02-11T17:31:38.880876",
"user_id":"default"}